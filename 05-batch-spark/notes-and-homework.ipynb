{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export JAVA_HOME=\"${HOME}/spark/jdk-11.0.2\"\n",
    "!export PATH=\"${JAVA_HOME}/bin:${PATH}\"\n",
    "!export SPARK_HOME=\"${HOME}/spark/spark-3.4.2-bin-hadoop3-scala2.13\"\n",
    "!export PATH=\"${SPARK_HOME}/bin:${PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is built for parallel data processing across machine clusters. It supports languages like Java, Python, R, and Scala. Spark includes libraries for a different tasks, such as SQL, streaming, and machine learning. It's adaptable, being possible to run from a laptop to clusters of thousands of servers. This makes Spark beginner-friendly, yet it comes with a steep learning curve.\n",
    "\n",
    "The Apache Spark framework operates on a master-slave architecture, comprising two key components: a **driver program** and **executors** nodes. The term \"nodes\" refers to individual computers or servers that are part of a larger network or cluster. Each node is an independent unit with its own CPU, memory, and storage, capable of executing tasks. To illustrate this, consider the following diagram for a higher overview of the architecture:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/architecture-spark.png\">\n",
    "</center>\n",
    "\n",
    "- **Driver program:** This is the heart of the Spark application, running the main function and creating the **SparkContext**. The **SparkContext** is essential for establishing communication with the **cluster manager** and creating Resilient Distributed Datasets (RDDs), which are distributed across the cluster and operated in parallel. Also, Spark Driver contains various other components such as DAG Scheduler, Task Scheduler, Backend Scheduler, and Block Manager, which are responsible for translating the user-written code into jobs that are actually executed on the cluster.\n",
    "\n",
    "- **Cluster Manager:** It's a service that can run on a node or across multiple nodes. The **cluster manager** is an external service for acquiring resources on the cluster (e.g., Apache Mesos, Hadoop YARN, or Spark's own standalone **cluster manager**). It allocates resources to Spark applications based on the requirements sent by the **Driver Program** through the **SparkContext**. It's responsible for allocating executor processes across the available worker nodes within the cluster as requested by the **Driver Program**.\n",
    "\n",
    "- **Worker Node:** This is a node where Spark executors run **tasks**. Each **worker node** is a physical or virtual machine that is part of the Spark cluster. \n",
    "    \n",
    "    - **Executors:** Are processes that run computations and store data for your application on the **worker node**. Each executor can run multiple tasks concurrently using threads.\n",
    "    - **Tasks:** Tasks are individual units of work sent to the **executor** by the driver. Each task corresponds to a combination of data and computation.\n",
    "    - **Cache:** This is used by executors to store data that can be reused in other **tasks**, reducing the need to fetch this data from disk or over the network for subsequent **tasks**.\n",
    "\n",
    "The **SparkContext** in the **driver program** requests resources from the **cluster manager**, which then assigns **worker nodes**. Executors on the **worker nodes** then process **tasks** and store results. These results are sent back to the **driver program**, which may then initiate further **tasks** or return results to the user.\n",
    "\n",
    "We can run Spark in three different modes: local mode, cluster mode, and client mode. The mode in which Spark runs determines the location of the **driver program** and the executors. The mode also affects how the **driver program** communicates with the **cluster manager** and the **worker nodes**. Consider each mode in more detail:\n",
    "\n",
    "### **Local Mode**\n",
    "\n",
    "Local mode simplifies the execution environment by running the entire Spark application on a single machine. Unlike cluster and client modes, which distribute **tasks** across multiple nodes in a cluster, local mode simulates a distributed environment using threads on one machine. This mode is ideal for development, testing, or experimentation, as it does not require a cluster and simplifies the setup process.\n",
    "\n",
    "### **Cluster Mode**\n",
    "\n",
    "The execution of a Spark application involves submitting the application to a **cluster manager** (such as Apache Mesos, Hadoop YARN, or Spark's own standalone **cluster manager**). Here, both the **driver program** and **executor** processes are launched within the cluster. Specifically, the driver process runs on one of the **worker nodes** designated by the **cluster manager**, separate from the machine where the submission occurred.\n",
    "\n",
    "### **Client Mode**\n",
    "\n",
    "Client mode operates similarly to cluster mode, with the key difference being the location of the Spark driver. In client mode, the driver runs on the client machine â€” the machine from which the application is submitted, often referred to as the gateway machine or edge node. This setup facilitates direct interaction between the application's user and the driver, making it easier to debug or monitor the application's progress. The **executor** processes, however, are still managed by the **cluster manager** and run within the cluster. Client mode is particularly useful during the development and debugging phases of an application, as it allows for immediate access to the driver's logs and prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Batch Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch processing in Spark executes jobs that handle large volumes of data in batches, contrasting with stream processing where data is processed continuously as it arrives. Each job processes a complete dataset at once.\n",
    "\n",
    "The workflow of batch processing in Spark begins by reading data in large batches from diverse data sources. This data is then processed in parallel across the nodes in the cluster, utilizing Spark's core abstractions such as RDDs (Resilient Distributed Datasets), DataFrames, or Datasets, depending on the specific requirements of the task. The data goes to a series of transformations, orchestrated through Directed Acyclic Graph (DAG) operations, scheduling the job's execution across the cluster.\n",
    "\n",
    "RDDs (Resilient Distributed Datasets), are a fundamental concept in Apache Spark, representing a distributed collection of objects. Internally, Spark DataFrames are built upon RDDs, introducing an additional layer of abstraction that facilitates more efficient data handling and processing. This is crucial for batch processing, where large datasets are divided into smaller partitions that are processed concurrently across multiple nodes, significantly speeding up processing times.\n",
    "\n",
    "Spark's approach to batch processing involves constructing a DAG of RDD transformations for each job, optimizing the execution plan to minimize data shuffling and enhance job execution times. The scheduler divides the DAG into stages that can be executed concurrently, further optimizing resource usage and processing time for batch jobs.\n",
    "\n",
    "The execution stages are divided into tasks, each designated for execution on an executor. Executors are basically JVM (java virtual machine) processes allocated to Spark applications by the cluster manager, running on worker nodes. The task scheduler efficiently distributes these tasks among the available executors, optimizing for data locality and balancing the workload to ensure the efficient execution of batch processing jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Virtual Machine in Google Cloud**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to run a Spark application in a virtual machine in Google Cloud in local and cluster mode. The first step is to create a virtual machine in Google Cloud.Before configuring the Virtual machine, we need to generate a ssh key pair. This will allow us to connect to the VM instance using the ssh protocol. To generate the key pair, open a terminal go to the `.ssh` directory and run the following command:\n",
    "\n",
    "```bash\n",
    "ssh-keygen -t rsa -f ~/.ssh/KEY_FILENAME -C USERNAME -b 2048\n",
    "```\n",
    "\n",
    "Where `KEY_FILENAME` is the name of the file that will store the key pair and `USERNAME` is the username that will be used to connect to the VM instance. The `-b` flag is used to specify the number of bits in the key. The default value is 2048 bits. This created a private key file `KEY_FILENAME` and a public key file `KEY_FILENAME.pub`. The public key file will be used to configure the VM instance.\n",
    "\n",
    "Now in the Google Cloud Plataform open the left bar and go to `Compute Engine > Metadata`, click on `SSH Keys` and `ADD SSH KEY` to add the public key to the list of SSH keys. The public key that need to be copied is the content of the `KEY_FILENAME.pub` file. After adding the key, click on `Save` to save the changes. At the end we should have the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/ssh-key.png\">\n",
    "</center>\n",
    "\n",
    "Now any Virtual machine that is created will use this ssh key. This will allow us to connect to the VM instance using the private key. Now we in the Google Cloud Platform, go to the navigation bar on the left and select `Compute Engine > VM instances` to create a new VM instance. Click on `Create` to create a new VM instance. \n",
    "\n",
    "**Machine configuration**\n",
    "\n",
    "The following image shows the configuration of the machine that will be used in this project.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/vm-setup.png\">\n",
    "</center>\n",
    "\n",
    "**Boot Disk**\n",
    "\n",
    "This will create a machine with 4 vCPUs and 16 GB of memory. The boot disk is a Ubuntu 22.04 LTS image with disk size of 40 GB. \n",
    "\n",
    "<center>\n",
    "<img src=\"figures/boot-disk.png\">\n",
    "</center>\n",
    "\n",
    "Then we can click on create to create the VM instance. We should see the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/vm-instance-ip.png\">\n",
    "</center>\n",
    "\n",
    "Now copy the external ip (EXTERNAL_IP) address of the VM instance and go to the terminal to connect using the following command:\n",
    "\n",
    "```bash\n",
    "ssh -i ~/.ssh/KEY_FILENAME USERNAME@EXTERNAL_IP\n",
    "```\n",
    "\n",
    "The `-i` flag is used to specify the private key file. After running the command, we should be connected to the VM instance. In my specific case i should connect using `ssh -i ~/.ssh/gcp_key marcos@34.138.143.219`. We can check the machine configuration using the htop command. The following image shows the output of the htop command.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/htop-vm.png\">\n",
    "</center>\n",
    "\n",
    "instead of using the entire ssh command, we can simplify by creating a `config` file in `.ssh` directory. The file should have the following content for each new ssh connection:\n",
    "\n",
    "```bash\n",
    "Host VM_NAME\n",
    "  HostName EXTERNAL_IP\n",
    "  User USERNAME\n",
    "  IdentityFile ~/.ssh/KEY_FILENAME\n",
    "```\n",
    "\n",
    "For my particular case, to connect to google Cloud, this would be:\n",
    "\n",
    "```bash\n",
    "Host de-bootcamp\n",
    "  HostName 34.138.143.219\n",
    "  User marcos\n",
    "  IdentityFile ~/.ssh/gcp_key\n",
    "```\n",
    "\n",
    "then we can connect to the VM instance simply using:\n",
    "\n",
    "```bash\n",
    "ssh VM_NAME\n",
    "```\n",
    "\n",
    "We can also configure our VScode to connect via ssh to the VM instance. To do this, we need to install the `Remote - SSH` extension. After installing the extension, click on the green icon on the bottom left of the VScode and select `Open a Remote Window:Connect to Host...`. After selecting the correct connection, the VScode will connect to the VM instance and we can start coding in the VM instance. For more details check [this](https://code.visualstudio.com/docs/remote/ssh). We should have something like:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/vscode-ssh.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Spark Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install Spark on the Virtual Machine is similar to the installation in a local computer with ubuntu via terminal. We first install Java and then Spark and Pyspark.\n",
    "\n",
    "### **java Installation**\n",
    "\n",
    "To install java, download OpenJDK 11 in [OpenJDK](https://jdk.java.net/archive/) and download it to `~/spark` folder and Unpack it with:\n",
    "\n",
    "```bash\n",
    "wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n",
    "\n",
    "tar xzfv openjdk-11.0.2_linux-x64_bin.tar.gz\n",
    "```\n",
    "the flag `xzfv` means to extract, decompress the archive using gzip before extracting, indicate that the next argument is the name of the file and print the names of files as they are extracted, respectively. In the `.bashrc` we need to define `JAVA_HOME` and add it to `PATH` to make it available to the system when the terminal are opened. Go to the `~/.bashrc` file and add the following:\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=\"${HOME}/spark/jdk-11.0.2\"\n",
    "export PATH=\"${JAVA_HOME}/bin:${PATH}\"\n",
    "```\n",
    "\n",
    "check that it works:\n",
    "\n",
    "```bash\n",
    "java --version\n",
    "```\n",
    "\n",
    "### **Spark Installation**\n",
    "\n",
    "\n",
    "To install Spark, download [Spark](https://spark.apache.org/downloads.html) to the same folder `~/spark` and unpack it with:\n",
    "\n",
    "```bash\n",
    "wget https://dlcdn.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3-scala2.13.tgz\n",
    "\n",
    "tar xzfv spark-3.4.2-bin-hadoop3-scala2.13.tgz\n",
    "```\n",
    "\n",
    "Again, in the `.bashrc` add a new path for spark to `PATH`:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=\"${HOME}/spark/spark-3.4.2-bin-hadoop3-scala2.13\"\n",
    "export PATH=\"${SPARK_HOME}/bin:${PATH}\"\n",
    "```\n",
    "\n",
    "To check if it works, open a new terminal, execute `spark-shell` and run the following:\n",
    "\n",
    "```bash\n",
    "val data = 1 to 10000\n",
    "val distData = sc.parallelize(data)\n",
    "distData.filter(_ < 10).collect()\n",
    "```\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/spark-test.png\" >\n",
    "</center>\n",
    "\n",
    "### **Pyspark Installation**\n",
    "\n",
    "To install `pyspark` we can use the following command:\n",
    "\n",
    "```bash\n",
    "pip install pyspark==3.4.2\n",
    "```\n",
    "make sure that the version match the version of the Spark installed, otherwise this can cause conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Testing Spark Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting up a development environment with VSCode to work with Jupyter notebooks, we might encounter a situation where VSCode does not automatically recognize environment variables such as JAVA_HOME and SPARK_HOME. In this case, if we are running in our local machine, we can open VScode via terminal by using `code`. This will allow VSCode to recognize the environment variables. But, if we are running in a VM instance from Google Cloud, for example, and connecting the VScode by the Remote-SSH application, a practical solution involves using port forwarding to access the Jupyter notebook through a web browser. Here I add port 8888 is for the jupyter notebook and port 4040  for the Spark UI, as show below:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/port-vscode.png\">\n",
    "</center>\n",
    "\n",
    "after this, make sure to have jupyter installed in the VM instance. To open the jupyter notebook, go to the terminal, connect with the VM instance and run the following:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "We should have something like:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/run-jupyter-terminal.png\">\n",
    "</center>\n",
    "\n",
    "This will be running in port `http://localhost:8888/`. The server provides a URL that includes a token-based authentication part, which looks like `?token=<some_long_string>`. This token is a security measure to prevent unauthorized access to our Jupyter notebooks, and in this case the HTTP request address is `http://localhost:8888/?token=f4c5f14db0b2934c18e2a911c85270067f6f7ba79f677510`. For the Spark UI we just need to access the address `http://localhost:4040`.\n",
    "\n",
    "Another approach to directly use VScode to run the Jupyter notebook with Spark, without the need to start the Jupyter UI in browser, would be starting the notebook by defining the environment variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export JAVA_HOME=\"${HOME}/spark/jdk-11.0.2\"\n",
    "!export PATH=\"${JAVA_HOME}/bin:${PATH}\"\n",
    "!export SPARK_HOME=\"${HOME}/spark/spark-3.4.2-bin-hadoop3-scala2.13\"\n",
    "!export PATH=\"${SPARK_HOME}/bin:${PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Now download the Taxi Zone Lookup CSV file from [NYC taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and save inside the `data` directory :\n",
    "\n",
    "```bash\n",
    "wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initiates a `SparkSession`, an entry point for programming Spark with the Dataset and DataFrame API. It is used to create DataFrames, register DataFrames as tables, execute SQL over tables, cache tables, and read parquet files. As we have seen before, Spark has three modes: cluster, client, and local. For testing, let's use the local mode by setting `.master(\"local[*]\")`. This means that Spark will run locally on the virtual machine instance from Google Cloud, using as many worker threads as there are logical cores on the machine. The method `.appName(\"test\")` sets the name of the application to be shown in the Spark web UI, and the method `.getOrCreate()` creates the SparkSession with all the parameters set previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Suppress warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the taxi zone lookup and show\n",
    "# Set header to true to consider first row as header\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/raw/taxi+_zone_lookup.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the `df.write.parquet('data/zones')` command in PySpark, the resulting `zones` directory will contains several files due to how Spark manages data storage in Parquet format. The `_SUCCESS` file is a marker indicating the write operation completed successfully. Accompanying CRC (Cyclic Redundancy Check) files, like `._SUCCESS.crc`, provide cyclic redundancy checks for data integrity verification. The data itself is stored in `.snappy.parquet` files, with their names including a unique identifier and a partition index, indicating they contain a portion of the DataFrame's data compressed using Snappy for efficiency. Each `.snappy.parquet` file also has a corresponding `.crc` file for integrity checks. The presence of these files reflects Spark's distributed data processing approach, where data can be partitioned across multiple files for scalable processing and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes the data in df to the zones directory in Parquet format\n",
    "df.write.parquet('data/zones', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Spark DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Transformation vs Action**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, operations on DataFrames are categorized into transformations and actions, which are fundamental to understanding how Spark processes data, especially in a distributed manner. Transformations are operations that create a new DataFrame from an existing one but do not trigger computation by themselves. Instead, they build up internally as a Directed Acyclic Graph (DAG) that Spark uses to compute the result lazily. Actions, in contrast, trigger the execution of the computations specified by the DAG of transformations. They are operations that produce a result.\n",
    "\n",
    "### **Transformation (Lazy)**\n",
    "Transformations create a new DataFrame or RDD from an existing one. \n",
    "\n",
    "1. **`filter(condition)`**: Returns a new DataFrame containing only the rows that meet the condition.\n",
    "   \n",
    "2. **`select(*cols)`**: Projects a set of expressions and returns a new DataFrame with the selected columns.\n",
    "   \n",
    "3. **`groupBy(*cols)`**: Groups the DataFrame using the specified columns, returning a GroupedData object that can be further aggregated.\n",
    "   \n",
    "4. **`orderBy(*cols, **kwargs)`**: Returns a new DataFrame sorted by the specified column(s).\n",
    "   \n",
    "5. **`join(other, on=None, how=None)`**: Joins with another DataFrame, using the given join expression and method.\n",
    "   \n",
    "6. **`withColumn(colName, col)`**: Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "   \n",
    "\n",
    "### **Actions (Eager)**\n",
    "Actions trigger the execution of the DAG of transformations and return a result\n",
    "\n",
    "1. **`show(n=20)`**: Displays the first `n` rows of the DataFrame.\n",
    "   \n",
    "2. **`count()`**: Returns the number of rows in the DataFrame.\n",
    "   \n",
    "3. **`collect()`**: Returns all the rows as a list of Row objects.\n",
    "   \n",
    "4. **`take(n)`**: Returns the first `n` rows as a list of Row objects.\n",
    "   \n",
    "5. **`first()`**: Returns the first row as a Row object.\n",
    "\n",
    "\n",
    "For better understand this concepts, for this part download the High Volume For-Hire Vehicle Trip Records (PARQUET) from 2023 in [NYC taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and save inside the `data` directory :\n",
    "\n",
    "```bash\n",
    "wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet\n",
    "```\n",
    "\n",
    "Let's start the SparkSession if not yet started and print the schema from parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp_ntz (nullable = true)\n",
      " |-- on_scene_datetime: timestamp_ntz (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Session if not\n",
    "#spark = SparkSession.builder \\\n",
    "#    .master(\"local[*]\") \\\n",
    "#    .appName('test') \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet('data/raw/fhvhv_tripdata_2023-01.parquet')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this transformation and actions in practice, let's select the `pickup_datetime`, `dropoff_datetime`, `PULocationID` and  `DOLocationID` columns filtering by  `hvfhs_license_num == 'HV0003` and show the result. The following diagram shows the DAG of transformations and actions:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/transf-actions.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2023-01-01 00:19:38|2023-01-01 00:48:07|          48|          68|\n",
      "|2023-01-01 00:58:39|2023-01-01 01:33:08|         246|         163|\n",
      "|2023-01-01 00:20:27|2023-01-01 00:37:54|           9|         129|\n",
      "|2023-01-01 00:41:05|2023-01-01 00:48:16|         129|         129|\n",
      "|2023-01-01 00:52:47|2023-01-01 01:04:51|         129|          92|\n",
      "|2023-01-01 00:10:29|2023-01-01 00:18:22|          90|         231|\n",
      "|2023-01-01 00:22:10|2023-01-01 00:33:14|         125|         246|\n",
      "|2023-01-01 00:39:09|2023-01-01 01:03:50|          68|         231|\n",
      "|2023-01-01 00:14:35|2023-01-01 00:49:13|          79|          50|\n",
      "|2023-01-01 00:52:15|2023-01-01 01:31:11|         143|         223|\n",
      "|2023-01-01 00:24:48|2023-01-01 00:37:39|          49|         181|\n",
      "|2023-01-01 00:46:20|2023-01-01 00:52:51|         181|          25|\n",
      "|2023-01-01 00:53:40|2023-01-01 01:31:23|          25|         143|\n",
      "|2023-01-01 00:28:05|2023-01-01 00:37:45|         223|           7|\n",
      "|2023-01-01 00:40:51|2023-01-01 00:54:09|           7|         223|\n",
      "|2023-01-01 00:59:56|2023-01-01 01:18:47|         223|         145|\n",
      "|2023-01-01 00:18:26|2023-01-01 00:30:48|         225|          61|\n",
      "|2023-01-01 00:33:32|2023-01-01 00:48:48|          61|          65|\n",
      "|2023-01-01 00:59:04|2023-01-01 01:13:50|          33|          80|\n",
      "|2023-01-01 00:48:17|2023-01-01 01:42:33|          19|          21|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID' )\\\n",
    "    .filter(df.hvfhs_license_num == 'HV0003')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Preparing Yellow and Green Taxi Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Download Data via Bash Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  Redefining Parquet Schema and Using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even our data being in parquet file, it's important to standardizing the schema across all datasets to ensures that the data types are exactly as expected for all periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "\n",
    "# Start Session if not\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName('test')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some piece of data to check the schema that comes with the taxi trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow schema\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{StructField('DOLocationID', LongType(), True),\n",
       " StructField('PULocationID', LongType(), True),\n",
       " StructField('RatecodeID', DoubleType(), True),\n",
       " StructField('VendorID', LongType(), True),\n",
       " StructField('airport_fee', DoubleType(), True),\n",
       " StructField('congestion_surcharge', DoubleType(), True),\n",
       " StructField('extra', DoubleType(), True),\n",
       " StructField('fare_amount', DoubleType(), True),\n",
       " StructField('improvement_surcharge', DoubleType(), True),\n",
       " StructField('mta_tax', DoubleType(), True),\n",
       " StructField('passenger_count', DoubleType(), True),\n",
       " StructField('payment_type', LongType(), True),\n",
       " StructField('store_and_fwd_flag', StringType(), True),\n",
       " StructField('tip_amount', DoubleType(), True),\n",
       " StructField('tolls_amount', DoubleType(), True),\n",
       " StructField('total_amount', DoubleType(), True),\n",
       " StructField('tpep_dropoff_datetime', TimestampNTZType(), True),\n",
       " StructField('tpep_pickup_datetime', TimestampNTZType(), True),\n",
       " StructField('trip_distance', DoubleType(), True)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green schema\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{StructField('DOLocationID', LongType(), True),\n",
       " StructField('PULocationID', LongType(), True),\n",
       " StructField('RatecodeID', DoubleType(), True),\n",
       " StructField('VendorID', LongType(), True),\n",
       " StructField('congestion_surcharge', DoubleType(), True),\n",
       " StructField('ehail_fee', IntegerType(), True),\n",
       " StructField('extra', DoubleType(), True),\n",
       " StructField('fare_amount', DoubleType(), True),\n",
       " StructField('improvement_surcharge', DoubleType(), True),\n",
       " StructField('lpep_dropoff_datetime', TimestampNTZType(), True),\n",
       " StructField('lpep_pickup_datetime', TimestampNTZType(), True),\n",
       " StructField('mta_tax', DoubleType(), True),\n",
       " StructField('passenger_count', DoubleType(), True),\n",
       " StructField('payment_type', DoubleType(), True),\n",
       " StructField('store_and_fwd_flag', StringType(), True),\n",
       " StructField('tip_amount', DoubleType(), True),\n",
       " StructField('tolls_amount', DoubleType(), True),\n",
       " StructField('total_amount', DoubleType(), True),\n",
       " StructField('trip_distance', DoubleType(), True),\n",
       " StructField('trip_type', DoubleType(), True)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_yellow_2022_01 = spark.read.parquet('data/raw/yellow/2022/01/')\n",
    "print(\"yellow schema\\n\")\n",
    "display(set(df_yellow_2022_01.schema))\n",
    "\n",
    "df_green_2022_01 = spark.read.parquet('data/raw/green/2022/01/')\n",
    "print(\"green schema\\n\")\n",
    "display(set(df_green_2022_01.schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we changed the types of `VendorID`, `DOLocationID`, `PULocationID`, `RatecodeID`, `passenger_count`, `payment_type` to `IntegerType`, `tpep_pickup_datetime` and `tpep_dropoff_datetime` to `TimestampType`. The script iterates over two years (2022 and 2023) and all months within those years, processing the datasets stored in parquet format. Then the data is redistribute across the Spark cluster by using `.repartition( )`. This will help to optimize the performance of the Spark job by ensuring that the data is evenly distributed across the available nodes in the cluster to be processed in parallel. The data is then saved in the parquet format with the new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2023/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for year in [2022, 2023]:\n",
    "    for month in range(1, 13):\n",
    "        print(f'processing data for {year}/{month}')\n",
    "\n",
    "        input_path = f'data/raw/yellow/{year}/{month:02d}/'\n",
    "        output_path = f'data/processed/new-schema/yellow/{year}/{month:02d}/'\n",
    "\n",
    "        df_yellow = spark.read.parquet(input_path)\n",
    "        df_yellow = df_yellow\\\n",
    "                    .withColumn(\"DOLocationID\",df_yellow[\"DOLocationID\"].cast(types.IntegerType()) )\\\n",
    "                    .withColumn(\"PULocationID\",df_yellow[\"PULocationID\"].cast(types.IntegerType()) )\\\n",
    "                    .withColumn(\"RatecodeID\",df_yellow[\"RatecodeID\"].cast(types.IntegerType()) )\\\n",
    "                    .withColumn(\"VendorID\",df_yellow[\"VendorID\"].cast(types.IntegerType()) )\\\n",
    "                    .withColumn(\"airport_fee\",df_yellow[\"airport_fee\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"congestion_surcharge\",df_yellow[\"congestion_surcharge\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"extra\",df_yellow[\"extra\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"fare_amount\",df_yellow[\"fare_amount\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"improvement_surcharge\",df_yellow[\"improvement_surcharge\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"mta_tax\",df_yellow[\"mta_tax\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"passenger_count\",df_yellow[\"passenger_count\"].cast(types.IntegerType()) )\\\n",
    "                    .withColumn(\"payment_type\",df_yellow[\"payment_type\"].cast(types.IntegerType()) )\\\n",
    "                    .withColumn(\"store_and_fwd_flag\",df_yellow[\"store_and_fwd_flag\"].cast(types.StringType()) )\\\n",
    "                    .withColumn(\"tip_amount\",df_yellow[\"tip_amount\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"tolls_amount\",df_yellow[\"tolls_amount\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"total_amount\",df_yellow[\"total_amount\"].cast(types.DoubleType()) )\\\n",
    "                    .withColumn(\"tpep_dropoff_datetime\",df_yellow[\"tpep_dropoff_datetime\"].cast(types.TimestampType()) )\\\n",
    "                    .withColumn(\"tpep_pickup_datetime\",df_yellow[\"tpep_pickup_datetime\"].cast(types.TimestampType()) )\\\n",
    "                    .withColumn(\"trip_distance\",df_yellow[\"trip_distance\"].cast(types.DoubleType()))            \n",
    "                \n",
    "        df_yellow\\\n",
    "            .repartition(4) \\\n",
    "            .write.mode('overwrite')\\\n",
    "            .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for 2022/2\n",
      "processing data for 2022/3\n",
      "processing data for 2022/4\n",
      "processing data for 2022/5\n",
      "processing data for 2022/6\n",
      "processing data for 2022/7\n",
      "processing data for 2022/8\n",
      "processing data for 2022/9\n",
      "processing data for 2022/10\n",
      "processing data for 2022/11\n",
      "processing data for 2022/12\n",
      "processing data for 2023/1\n",
      "processing data for 2023/2\n",
      "processing data for 2023/3\n",
      "processing data for 2023/4\n",
      "processing data for 2023/5\n",
      "processing data for 2023/6\n",
      "processing data for 2023/7\n",
      "processing data for 2023/8\n",
      "processing data for 2023/9\n",
      "processing data for 2023/10\n",
      "processing data for 2023/11\n",
      "processing data for 2023/12\n"
     ]
    }
   ],
   "source": [
    "for year in [2022, 2023]:\n",
    "    for month in range(1, 13):\n",
    "        print(f'processing data for {year}/{month}')\n",
    "\n",
    "        input_path = f'data/raw/green/{year}/{month:02d}/'\n",
    "        output_path = f'data/processed/new-schema/green/{year}/{month:02d}/'\n",
    "        \n",
    "        df_green = spark.read.parquet(input_path)\n",
    "        df_green = df_green\\\n",
    "            .withColumn(\"DOLocationID\", df_green[\"DOLocationID\"].cast(types.IntegerType()))\\\n",
    "            .withColumn(\"PULocationID\", df_green[\"PULocationID\"].cast(types.IntegerType()))\\\n",
    "            .withColumn(\"RatecodeID\", df_green[\"RatecodeID\"].cast(types.IntegerType()))\\\n",
    "            .withColumn(\"VendorID\", df_green[\"VendorID\"].cast(types.IntegerType()))\\\n",
    "            .withColumn(\"congestion_surcharge\", df_green[\"congestion_surcharge\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"ehail_fee\", df_green[\"ehail_fee\"].cast(types.IntegerType()))\\\n",
    "            .withColumn(\"extra\", df_green[\"extra\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"fare_amount\", df_green[\"fare_amount\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"improvement_surcharge\", df_green[\"improvement_surcharge\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"lpep_dropoff_datetime\", df_green[\"lpep_dropoff_datetime\"].cast(types.TimestampType()))\\\n",
    "            .withColumn(\"lpep_pickup_datetime\", df_green[\"lpep_pickup_datetime\"].cast(types.TimestampType()))\\\n",
    "            .withColumn(\"mta_tax\", df_green[\"mta_tax\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"passenger_count\", df_green[\"passenger_count\"].cast(types.IntegerType()))\\\n",
    "            .withColumn(\"payment_type\", df_green[\"payment_type\"].cast(types.IntegerType()))\\\n",
    "            .withColumn(\"store_and_fwd_flag\", df_green[\"store_and_fwd_flag\"].cast(types.StringType()))\\\n",
    "            .withColumn(\"tip_amount\", df_green[\"tip_amount\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"tolls_amount\", df_green[\"tolls_amount\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"total_amount\", df_green[\"total_amount\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"trip_distance\", df_green[\"trip_distance\"].cast(types.DoubleType()))\\\n",
    "            .withColumn(\"trip_type\", df_green[\"trip_type\"].cast(types.DoubleType()))\n",
    "        \n",
    "        df_green\\\n",
    "            .repartition(4) \\\n",
    "            .write.mode('overwrite')\\\n",
    "            .parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the entire dataset taking into account all years for each taxi service. The transformation in the schema ensure that all data is consistent and can be used for further analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows for yellow taxi: 77966324\n",
      "\n",
      "+-------------------+-------------------+---------------+-------------+\n",
      "|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|\n",
      "+-------------------+-------------------+---------------+-------------+\n",
      "|2022-10-30 13:20:54|2022-10-30 13:35:28|           null|          0.0|\n",
      "|2022-10-20 11:28:50|2022-10-20 11:43:51|              2|         2.32|\n",
      "|2022-10-22 17:05:36|2022-10-22 17:17:40|              1|          1.8|\n",
      "|2022-10-20 13:36:58|2022-10-20 13:47:52|              3|          0.9|\n",
      "|2022-10-13 09:49:44|2022-10-13 10:43:58|              2|         7.83|\n",
      "+-------------------+-------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows for green taxi: 1627462\n",
      "\n",
      "+-------------------+-------------------+---------------+-------------+\n",
      "|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|\n",
      "+-------------------+-------------------+---------------+-------------+\n",
      "|2022-03-22 20:32:55|2022-03-22 20:35:40|              1|         0.54|\n",
      "|2022-03-24 20:52:32|2022-03-24 20:56:31|              1|         0.86|\n",
      "|2022-03-22 18:41:31|2022-03-22 19:05:08|              1|          8.2|\n",
      "|2022-03-29 09:26:54|2022-03-29 09:43:17|              1|          3.5|\n",
      "|2022-03-07 16:23:16|2022-03-07 16:30:23|              1|          1.0|\n",
      "+-------------------+-------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# All years and months from green and yellow taxi\n",
    "df_yellow = spark.read.parquet('data/processed/new-schema/yellow/*/*')\n",
    "df_green = spark.read.parquet('data/processed/new-schema/green/*/*/')\n",
    "\n",
    "\n",
    "# Rename columns so the green and yellow taxi match\n",
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "    \n",
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n",
    "\n",
    "\n",
    "    \n",
    "print(f'number of rows for yellow taxi: {df_yellow.count()}\\n')\n",
    "display(df_yellow.select(\"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\").show(5))\n",
    "print(f'number of rows for green taxi: {df_green.count()}\\n')\n",
    "display(df_green.select(\"pickup_datetime\", \"dropoff_datetime\", \"passenger_count\", \"trip_distance\").show(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select just the common columns between the yellow and green taxi datasets and union them together, identifying the taxi service type by adding a new column called `service_type`. Then, we can take some summary statistic using SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:==================================================>    (23 + 2) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|count(1)|\n",
      "+------------+--------+\n",
      "|       green| 1627462|\n",
      "|      yellow|77966324|\n",
      "+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# To maintain the order of the columns\n",
    "common_columns = []\n",
    "\n",
    "yellow_columns = set(df_yellow.columns)\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_columns.append(col)\n",
    "\n",
    "#print(f'\\n{common_columns}\\n')\n",
    "\n",
    "# Select common columns and\n",
    "# Add service_type column (yellow or green)\n",
    "df_green_select = df_green \\\n",
    "    .select(common_columns) \\\n",
    "    .withColumn('service_type', F.lit('green'))\n",
    "\n",
    "df_yellow_select = df_yellow \\\n",
    "    .select(common_columns) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))\n",
    "    \n",
    "df_trips_data = df_green_select.unionAll(df_yellow_select)\n",
    "\n",
    "df_trips_data.createOrReplaceTempView('trips_data')\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    service_type,\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY \n",
    "    service_type\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge',\n",
       " 'service_type']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 286:====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------+--------------------+\n",
      "|revenue_zone|      revenue_month|service_type|revenue_monthly_fare|\n",
      "+------------+-------------------+------------+--------------------+\n",
      "|          92|2022-03-01 00:00:00|       green|            10347.98|\n",
      "|          80|2022-04-01 00:00:00|       green|             5414.24|\n",
      "|         109|2022-04-01 00:00:00|       green|                25.0|\n",
      "|          82|2022-06-01 00:00:00|       green|  32203.450000000004|\n",
      "|          72|2022-06-01 00:00:00|       green|  1744.9600000000007|\n",
      "+------------+-------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")\n",
    "\n",
    "display(df_result.select(\"revenue_zone\", \"revenue_month\", \"service_type\", \"revenue_monthly_fare\").show(5))\n",
    "\n",
    "df_result.coalesce(1).write.parquet('data/report/revenue/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Group By and Joins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the mechanics of a GROUP BY in Spark is crucial. Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('data/processed/new-schema/green/*/*/')\n",
    "df_green.registerTempTable('green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, we perform a GROUP BY query that aggregates total revenue and counts the records by hour and zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 316:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------+--------------+\n",
      "|               hour|zone|amount|number_records|\n",
      "+-------------------+----+------+--------------+\n",
      "|2022-01-01 00:00:00|   7|   9.8|             1|\n",
      "|2022-01-01 00:00:00|  25| 25.05|             1|\n",
      "|2022-01-01 00:00:00|  33|158.72|             4|\n",
      "|2022-01-01 00:00:00|  37| 37.95|             2|\n",
      "|2022-01-01 00:00:00|  40|   7.3|             1|\n",
      "+-------------------+----+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_green_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2022-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "ORDER BY\n",
    "    1, 2\n",
    "\"\"\")\n",
    "\n",
    "display(df_green_revenue.show(5))\n",
    "\n",
    "df_green_revenue.write.parquet('data/report/revenue/green', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"figures/groupby-dag.png\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAG visualization, in the `Stage 10` represents the initial action of reading data from a Parquet file. This stage was skipped, because the data was already read and is available in memory. The `WholeStageCoden` is an optimization technique that compiles an entire query stage into a single Java function, which reduces virtual function calls and CPU instruction cache misses. It encompasses the Scan Parquet operation. Finally, the `Exchange` is a shuffle operation, which redistributes data across different executors based on a partitioning scheme. This is required for operations like `GROUP BY` that need to aggregate data that could be distributed across different partitions.\n",
    "\n",
    "The `Stage 11` represents an adaptive query execution shuffle read. It's part of Spark's adaptive query execution, where it reads shuffled data. This stage was also skipped because we previously run a GROUP BY query, making the shuffle data was already available.\n",
    "\n",
    "and finally, the `Stage 12` is similar to the previous shuffle read, but this is part of the stage that wasn't skipped, indicating that this shuffle read is part of preparing data for the final ORDER BY and write operations. The WriteFiles is the final stage where the processed data is written out to a Parquet file in the specified location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the JOIN, there is two types, joining two large tables and joining one large table with a small one. consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('data/processed/new-schema/yellow/*/*/')\n",
    "df_yellow.registerTempTable('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_revenue = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('hour', tpep_pickup_datetime) AS hour,\n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    yellow\n",
    "WHERE\n",
    "    tpep_pickup_datetime >= '2022-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_revenue\\\n",
    "    .repartition(20)\\\n",
    "    .write.parquet('data/report/revenue/yellow', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make clear which column is from each table\n",
    "df_green_revenue_tmp = df_green_revenue\\\n",
    "    .withColumnRenamed('amount', 'green_amount')\\\n",
    "    .withColumnRenamed('number_records', 'green_number_records')\n",
    "\n",
    "df_yellow_revenue_tmp = df_yellow_revenue\\\n",
    "    .withColumnRenamed('amount', 'yellow_amount')\\\n",
    "    .withColumnRenamed('number_records', 'yellow_number_records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join record that is in green but not in yellow\n",
    "df_join = df_green_revenue.join(df_yellow_revenue, on= ['hour', 'zone'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.write('data/report/revenue/yellow', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 448:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------+--------------+------------------+--------------+\n",
      "|               hour|zone|amount|number_records|            amount|number_records|\n",
      "+-------------------+----+------+--------------+------------------+--------------+\n",
      "|2022-01-01 00:00:00|   4|  null|          null|203.73000000000002|            11|\n",
      "|2022-01-01 00:00:00|   7|   9.8|             1|             92.12|             6|\n",
      "|2022-01-01 00:00:00|  33|158.72|             4|             89.55|             5|\n",
      "|2022-01-01 00:00:00|  40|   7.3|             1|              null|          null|\n",
      "|2022-01-01 00:00:00|  45|  null|          null|            205.46|            11|\n",
      "+-------------------+----+------+--------------+------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_join.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.write.parquet('data/report/revenue/yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"figures/join-dag1.png\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6.Running Spark in the Clound**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
