{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Install Spark Locally**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Installing java**\n",
    "\n",
    "First, download OpenJDK 11 or Oracle JDK 11 in [OpenJDK](https://jdk.java.net/archive/)\n",
    "\n",
    "Download it to `~/spark` folder with:\n",
    "\n",
    "```bash\n",
    "wget https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n",
    "```\n",
    "\n",
    "Unpack it. the flag `xzfv` means to extract, decompress the archive using gzip before extracting, indicate that the next argument is the name of the file and print the names of files as they are extracted, respectively:\n",
    "\n",
    "```bash\n",
    "tar xzfv openjdk-11.0.2_linux-x64_bin.tar.gz\n",
    "```\n",
    "\n",
    "In the `.bashrc` we need to define `JAVA_HOME` and add it to `PATH` to make it available to the system when the terminal are opened:\n",
    "\n",
    "```bash\n",
    "export JAVA_HOME=\"${HOME}/spark/jdk-11.0.2\"\n",
    "export PATH=\"${JAVA_HOME}/bin:${PATH}\"\n",
    "```\n",
    "\n",
    "check that it works:\n",
    "\n",
    "```bash\n",
    "java --version\n",
    "```\n",
    "\n",
    "2. **Installing Spark**\n",
    "\n",
    "\n",
    "Download [Spark](https://spark.apache.org/downloads.html) to `~/spark` folder:\n",
    "\n",
    "```bash\n",
    "wget https://dlcdn.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3-scala2.13.tgz\n",
    "```\n",
    "\n",
    "Unpack:\n",
    "\n",
    "```bash\n",
    "tar xzfv spark-3.4.2-bin-hadoop3-scala2.13.tgz\n",
    "```\n",
    "\n",
    "In the `.bashrc` add again a new path for spark to `PATH`:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=\"${HOME}/spark/spark-3.4.2-bin-hadoop3-scala2.13\"\n",
    "export PATH=\"${SPARK_HOME}/bin:${PATH}\"\n",
    "```\n",
    "\n",
    "3. **Testing Spark**\n",
    "\n",
    "Execute `spark-shell` and run the following:\n",
    "\n",
    "```scala\n",
    "val data = 1 to 10000\n",
    "val distData = sc.parallelize(data)\n",
    "distData.filter(_ < 10).collect()\n",
    "```\n",
    "\n",
    "<center>\n",
    "<img src=\"data/spark-test.png\" >\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we already have python, to run PySpark, we first need to add it to `PYTHONPATH`:\n",
    "\n",
    "```bash\n",
    "export PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\n",
    "export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\n",
    "```\n",
    "This tells the Python interpreter where to locate additional modules and packages that are not part of the standard library or not in the current directory. Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or will\n",
    "encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
    "\n",
    "or we could simply use pip to install the package `pyspark`:\n",
    "\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "This will automatically add the path to `PYTHONPATH`.\n",
    "\n",
    "\n",
    "\n",
    "Download a CSV file that we'll use for testing:\n",
    "\n",
    "```bash\n",
    "wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
    "```\n",
    "\n",
    "Now let's execute here the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/taxi+_zone_lookup.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Spark with Airflow using Dataproc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using Google Cloud Dataproc, you only need to have Airflow set up with the necessary operators to orchestrate your Spark jobs. Dataproc handles the Spark environment, so you don't need to manage Spark installations directly. Airflow will communicate with Dataproc to submit and manage Spark jobs, leveraging Dataproc's managed Spark clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
