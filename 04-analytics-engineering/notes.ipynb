{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outline**\n",
    "\n",
    "- [**1. Introduction to dbt**](#1.-Introduction-to-dbt)\n",
    "- [**2. Integration of dbt core within Airflow using Astro CLI**](#2.-Integration-of-dbt-core-within-Airflow-using-Astro-CLI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to dbt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discuss about dbt (data build tool), let's first understand the concepts of ETL and ELT. The ETL (Extract, Transform, Load) process is a traditional method of extracting raw data from sources, such as files or databases, transforming this data as necessary, and then loading it into a data warehouse.. On the other hand, the ELT (Extract, Load, Transform) process represents a modern approach, where raw data is first loaded into the data warehouse before being transformed within the warehouse itself. The following diagram can squematicaly represent the ETL and ELT process:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/etl-elt.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "This shift of using ETL instead of ELT is due to the increased processing capabilities of modern data warehouses, which can efficiently handle complex transformations. dbt simplifies the ELT process by enabling data analysts and engineers to define transformations in SQL. This approach allows for the modular, version-controlled, and testable transformation of raw data, making it ready for business use. The following table summarizes benefits of using ETL and ELT:\n",
    "\n",
    "| ETL                                   | ELT                                |\n",
    "|---------------------------------------|------------------------------------|\n",
    "| Slightly more stable                  | Faster and more flexible           |\n",
    "| Higher storage and compute costs      | Lower cost and lower maintenance   |\n",
    "\n",
    "\n",
    "There is two ways of using dbt. The first one is using dbt core, which is a command-line tool that enables data analysts and engineers to transform data in their warehouse more effectively. The second one is using dbt cloud, which is a cloud-based service that provides a user interface for dbt core, as well as additional features such as scheduling, monitoring, and collaboration. In this notes, we will focus on the dbt core with the integration of Airflow.\n",
    "\n",
    "The only dependency we need to install is `dbt-core`:\n",
    "```bash\n",
    "    pip install dbt-core\n",
    "```\n",
    "Making the `dbt init` command available to create a new dbt project with the necessary directory structure and template files. In cases where we would need to use the dbt core locally, without a containerized environment, we would need to install the database adapter for the database we are using, like postgres and bigquery:\n",
    "\n",
    "```bash\n",
    "    pip install dbt-postgres\n",
    "    pip install dbt-bigquery\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Integration of dbt core with Airflow using Astro CLI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Astro CLI is designed to help developers easily create, manage, and deploy Airflow projects. The Astro CLI can quickly generate a new Airflow project with the necessary configuration and folders, without the need of configuring all setting in the docker-compose file as would be required to run airflow with docker. The Astro CLI abstracts all this dificults and make it easy to run Airflow locally, by creating four docker containers for the Airflow webserver, scheduler, and triggerer, as well as a Postgres database. Another advantage of using the Astro CLI is that it allows for the integration of dbt core within Airflow, which is the main focus of this notes. This integration make obsolete the need of using the dbt clound, as we can run dbt core within Airflow, schedule dbt runs with Airflow's scheduler, create dbt models with Airflow's DAGs and leverage Airflow's UI to monitor dbt runs.\n",
    "\n",
    "To install Astro CLI in Linux, we can use the following command:\n",
    "\n",
    "```bash\n",
    "    curl -sSL https://install.astronomer.io | sudo bash\n",
    "```\n",
    "\n",
    "After installation, we can create a new Airflow project using the following command:\n",
    "\n",
    "```bash\n",
    "    astro dev init\n",
    "```\n",
    "\n",
    "This will create all the folder structure of airflow and the necessary files to configure and run the Airflow project. The folder structure and files are the following:\n",
    "\n",
    "```markdown\n",
    "airflow-project/\n",
    "    ├── .astro/\n",
    "    │   ├── config.yaml\n",
    "    ├── dags/\n",
    "    │   ├── .airflowignore\n",
    "    ├── include/\n",
    "    ├── plugins/\n",
    "    ├── tests/\n",
    "    │   ├── dags/\n",
    "    ├── .dockerignore\n",
    "    ├── .env                    # Environment variables\n",
    "    ├── .gitignore\n",
    "    ├── airflow_settings.yaml   # Setting the connections to databases like postgres\n",
    "    ├── Dockerfile\n",
    "    ├── packages.txt            # For OS-level packages to install    \n",
    "    ├── requirements.txt        # Python dependencies for Airflow\n",
    "```\n",
    "Now we have our Airflow project ready to run. We can check the documentation how to integrate the dbt with Airflow in [here](https://docs.astronomer.io/learn/airflow-dbt). When checking the documentation, it say that we need to add the following lines of code in Dockerfile:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "Dockerfile\n",
    "</font>\n",
    "\n",
    "```Dockerfile\n",
    "    RUN python -m venv dbt_venv && source dbt_venv/bin/activate && \\\n",
    "        pip install --no-cache-dir dbt-postgres && deactivate\n",
    "```\n",
    "\n",
    "instead of using pip install for each library, we can create a `dbt_requeriments.txt` file with many libraries we want for the database providers:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "dbt_requeriments.txt\n",
    "</font>\n",
    "\n",
    "```txt\n",
    "    dbt-core>=1.7.8\n",
    "    dbt-postgres>=1.7.8\n",
    "    dbt-bigquery>=1.7.6\n",
    "```\n",
    "make sure to also install the packages locally to run dbt commands outside the container:\n",
    "\n",
    "```bash\n",
    "    pip install -r dbt_requirements.txt\n",
    "```\n",
    "\n",
    "Now we can slightly change the Dockerfile to copy the `dbt_requirements.txt` file and install the packages in the virtual environment. The content inside the Dockerfile would be:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "Dockerfile\n",
    "</font>\n",
    "\n",
    "```Dockerfile\n",
    "    FROM quay.io/astronomer/astro-runtime:10.3.0\n",
    "\n",
    "    WORKDIR \"/usr/local/airflow\"\n",
    "\n",
    "    COPY dbt-requirements.txt ./\n",
    "    RUN python -m virtualenv dbt_venv && source dbt_venv/bin/activate && \\\n",
    "        pip install --no-cache-dir -r dbt_requirements.txt && deactivate\n",
    "```\n",
    "\n",
    "This way, we create a isolated Python environments, where `dbt_venv` is the name of the new virtual environment to be created. The `source dbt_venv/bin/activate` command activates the virtual environment, and the `pip install --no-cache-dir -r dbt-requirements.txt` command installs the packages listed in the `dbt-requirements.txt` file. The `deactivate` command deactivates the virtual environment. This way we are taking precautions so that the dbt not conflict with the other packages installed in the Airflow environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a new directory called `dbt` inside the `dag/` directory. The `dbt-core` can create automatically the folder structure and the files using the following command inside `dbt` directory:\n",
    "\n",
    "```bash\n",
    "    dbt init\n",
    "```\n",
    "\n",
    "Inside the `dag/dbt/` directory we will have the following structure:\n",
    "\n",
    "```markdown\n",
    "airflow-project/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── dbt-project/\n",
    "    │   │   │   ├── analyses/\n",
    "    │   │   │   ├── models/\n",
    "    │   │   │   ├── macros/\n",
    "    │   │   │   ├── seeds/\n",
    "    │   │   │   ├── snapshots/\n",
    "    │   │   │   ├── tests/\n",
    "    │   │   │   ├── `dbt_project.yml`\n",
    "    ├── include/\n",
    "    ├── plugins/\n",
    "    ├── tests/\n",
    "    ├── .dockerignore\n",
    "    ├── .env                    \n",
    "    ├── .gitignore\n",
    "    ├── airflow_settings.yaml   \n",
    "    ├── Dockerfile\n",
    "    ├── packages.txt               \n",
    "    ├── requirements.txt      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The Astro CLI is built on top of Docker Compose, a tool for defining and running multi-container Docker applications. To override the default CLI configurations, add a `docker-compose.override.yml` file to Astro project directory. The values in this file override the default settings when we run  `astro dev start`. This information can be found [here](https://github.com/astronomer/docs/blob/main/software/customize-image.md). The `docker-compose.override.yml` file is used to persist the connection between the Airflow and dbt, so that the dbt models can be created and run within the Airflow environment. The content of the `docker-compose.override.yml` file would be:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "docker-compose.override.yml\n",
    "</font>\n",
    "\n",
    "```yaml\n",
    "    version: \"3.1\"\n",
    "    services:\n",
    "    scheduler:\n",
    "        volumes:\n",
    "        - /home/user/.google/credentials/google_credentials.json:/usr/local/airflow/google/credentials/google_credentials.json:rw \n",
    "        - ./dags/dbt:/usr/local/airflow/dags/dbt:rw  \n",
    "\n",
    "    webserver:\n",
    "        volumes:\n",
    "        - /home/user/.google/credentials/google_credentials.json:/usr/local/airflow/google/credentials/google_credentials.json:rw\n",
    "        - ./dags/dbt:/usr/local/airflow/dbt:rw\n",
    "\n",
    "    triggerer:\n",
    "        volumes:\n",
    "        - /home/user/.google/credentials/google_credentials.json:/usr/local/airflow/google/credentials/google_credentials.json:rw\n",
    "        - ./dags/dbt:/usr/local/airflow/dags/dbt:rw\n",
    "```\n",
    "\n",
    "Each service is a container that runs a specific process, such as the webserver, scheduler, and triggerer. The volumes will be the same for all services, and the `volumes` option is used to mount the `google_credentials.json` file and the `dags/dbt` directory.\n",
    "\n",
    "The `google_credentials.json` file is the file that we download from the Google Cloud Platform when we create a new service account for a project. This is used to authenticate the Airflow environment with the Google Cloud Platform to use the Google Cloud SDK libraries. We also add to the `.env` file the following line to specify the path to the `google_credentials.json` file in the container:\n",
    "\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    ".env\n",
    "</font>\n",
    "\n",
    "```bash\n",
    "    GOOGLE_APPLICATION_CREDENTIALS = \"/usr/local/airflow/google/credentials/google_credentials.json\"\n",
    "```\n",
    "\n",
    "dbt will automatically read the `.env` file when required to access google cloud. The volume for `dags/dbt` is to automatically synchronize the dbt models with the Airflow environment. The `rw` option is used to give read and write permissions to the Airflow services. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We are almost there, the last step is to add packages to the `packages.txt` file. The `packages.txt` file is used to install OS-level packages in the Airflow environment. The content of the `packages.txt` file would be:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "packages.txt\n",
    "</font>\n",
    "\n",
    "```txt\n",
    "    gcc\n",
    "    python3-venv\n",
    "``` \n",
    "\n",
    "To properly set up our Airflow environment with the necessary astronomer-cosmos package and its specific database integrations, we include the following lines in our `requirements.txt` file:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "requirements.txt\n",
    "</font>\n",
    "\n",
    "```txt\n",
    "    apache-airflow-providers-google\n",
    "    astronomer-cosmos[dbt-bigquery]\n",
    "    astronomer-cosmos[dbt-postgres]\n",
    "    pyarrow\n",
    "```\n",
    "\n",
    "The packages  `astronomer-cosmos[dbt-bigquery]` and  `astronomer-cosmos[dbt-postgres]` will include all the necessary packages to run dbt within the Airflow environment with all the databases adapters for bigquery and postgres. The Google Cloud SDK libraries is instated with `apache-airflow-providers-google` package to authenticate the Airflow environment with the Google Cloud Platform. The `pyarrow` package is used to read and write parquet files in the Airflow environment.\n",
    "\n",
    "The last step is now to run the following command inside the root of the Airflow project to start the Airflow environment with the dbt integration:\n",
    "\n",
    "```bash\n",
    "    astro dev start\n",
    "```\n",
    "\n",
    "The final structure for the directory must be equal to the following:\n",
    "\n",
    "```markdown\n",
    "airflow-project/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── dbt-project/\n",
    "    │   │   │   ├── analyses/\n",
    "    │   │   │   ├── models/\n",
    "    │   │   │   ├── macros/\n",
    "    │   │   │   ├── seeds/\n",
    "    │   │   │   ├── snapshots/\n",
    "    │   │   │   ├── tests/\n",
    "    │   │   │   ├── dbt_project.yml\n",
    "    ├── include/\n",
    "    ├── plugins/\n",
    "    ├── tests/\n",
    "    ├── .dockerignore\n",
    "    ├── `.env`                   \n",
    "    ├── .gitignore\n",
    "    ├── airflow_settings.yaml   \n",
    "    ├── `dbt-requirements.txt`\n",
    "    ├── `Dockerfile`\n",
    "    ├── `docker-compose.override.yml`\n",
    "    ├── `packages.txt`               \n",
    "    ├── `requirements.txt`     \n",
    "```\n",
    "\n",
    "All edited files are colored. The `astro dev start` command will start the Airflow environment with the dbt integration. The Airflow environment will be available at `http://localhost:8080`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 dbt-core Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our models in dbt is good to have the local environment to run the dbt commands, like `dbt run` and `dbt seed`. For using dbt-core locally, its required to have a profile file in the `~/.dbt/` directory, for more info go to [dbt documentation](https://docs.getdbt.com/docs/core/connect-data-platform/connection-profiles). We can check where the dbt is looking for the profile file using the following command:\n",
    "\n",
    "```bash\n",
    "    dbt debug --config-dir\n",
    "```\n",
    "\n",
    "which should return something like:\n",
    "\n",
    "```bash\n",
    "    To view your profiles.yml file, run:\n",
    "\n",
    "    xdg-open /home/user/.dbt\n",
    "```\n",
    "\n",
    "The profile file is used to specify the connection to the database, and the credentials to authenticate the connection. The profile file is a YAML file that contains the following information, in this case for a bigquery database:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "profile.yml\n",
    "</font>\n",
    "\n",
    "```yaml\n",
    "    bigquery-db:\n",
    "        target: dev\n",
    "        outputs:\n",
    "            dev:\n",
    "                type: bigquery\n",
    "                host: service-account\n",
    "                project: project-id\n",
    "                dataset: dataset-id\n",
    "                threads: 1\n",
    "                keyfile: /home/user/.google/credentials/google_credentials.json\n",
    "```\n",
    "\n",
    "Inside the `dag/dbt/` directory we will have the following structure:\n",
    "\n",
    "```markdown\n",
    "airflow-project/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── dbt-project/\n",
    "    │   │   │   ├── analyses/\n",
    "    │   │   │   ├── models/\n",
    "    │   │   │   ├── macros/\n",
    "    │   │   │   ├── seeds/\n",
    "    │   │   │   ├── snapshots/\n",
    "    │   │   │   ├── tests/\n",
    "    │   │   │   ├── `dbt_project.yml`\n",
    "    ├── include/\n",
    "    ├── plugins/\n",
    "    ├── tests/\n",
    "    ├── .dockerignore\n",
    "    ├── .env                   \n",
    "    ├── .gitignore\n",
    "    ├── airflow_settings.yaml   \n",
    "    ├── dbt-requirements.txt\n",
    "    ├── Dockerfile\n",
    "    ├── docker-compose.override.yml\n",
    "    ├── packages.txt               \n",
    "    ├── requirements.txt   \n",
    "```\n",
    "\n",
    "Inside the file `dbt_project.yml` have the configuration of the dbt project, where we can specify the name of the project, the profile used for the project and others setups. The part we need to change in the `dbt_project.yml` file is the profile to match the profile name in the `~/.dbt/profiles.yml` file.\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "dbt_project.yml\n",
    "</font>\n",
    "\n",
    "```yaml\n",
    "    name: 'taxi_rides_ny'\n",
    "    version: '1.0.0'\n",
    "    config-version: 2\n",
    "\n",
    "    profile: 'bigquery-db'\n",
    "```\n",
    "\n",
    "Before running models that depend on seeds, we need to run the following command inside the project directory `dag/dbt/dbt-project/` at the terminal to load the seed data into the database:\n",
    "\n",
    "```bash\n",
    "    dbt seed\n",
    "```\n",
    "\n",
    "This command will create tables in our database from the CSV files in our seeds directory. To run a dbt models, we can use the following command inside the `dag/dbt/dbt-project/` directory:\n",
    "\n",
    "```bash\n",
    "    dbt run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Building a ELT Model with Airflow and dbt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build a model using three datasets, the `green_tripdata`, `yellow_tripdata` and the `taxi_zone_lookup` from [NYC taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). The green_tripdata and yellow_tripdata are the datasets that contains the taxi trips, and the taxi_zone_lookup is the dataset that contains the information about the taxi zones.The following diagram shows the structure of the dbt model:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/dbt-model.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "In the first level of this diagram we have the `green_taxi_external_2019` and `yellow_taxi_external_2019` as the datasets ingested into the warehouse and the `taxi_zone_lookup` as the seed data. The Seeds is the directory inside dbt directory where contain a static data that is typically not expected to change frequently. \n",
    "\n",
    "For the second level of this diagram we have the Staging models that serves as an intermediate layer in the data transformation process. They are responsible for ingesting raw data from the sources `green_taxi_external_2019` and `yellow_taxi_external_2019` performing basic transformations to prepare the data for further processing.\n",
    "\n",
    "For the third level of this diagram we have the `fact_trips` and `dimension_zones` models. The `fact` model is the model that contains the data that is being measured, and the `dimension` model is the model that contains the data that provides context for the measurements. The `fact` and `dimension` models are the final models that are used to create the reports and dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 DAG to ingest data into GCS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this project is from the [NYC taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) dataset. We are only interested in the  Yellow and Green taxi trips dataset from the year 2019 and the taxi zone lookup dataset. For the taxi trip the format of the data is `.parquet` and for the zone lookup is in `.csv` . \n",
    "\n",
    "To start this project, the idea is to create a pipeline in Airflow to ingest the taxi data into a bucket in Google Cloud Storage and then create a external table in BigQuery. This way we can use the BigQuery as the warehouse to store the data and make the transformation with dbt. For the taxi zone lookup we use as a `seed` data, which is a static data that is typically not expected to change frequently. The Seed is the directory inside dbt directory where contain the static data, shown in the directory structure below:\n",
    "\n",
    "```markdown\n",
    "astro-airflow/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── taxi_rides_ny/\n",
    "    │   │   │   ├── analyses/\n",
    "    │   │   │   ├── models/\n",
    "    │   │   │   ├── macros/\n",
    "    │   │   │   ├── seeds/\n",
    "    │   │   │   │   ├── `taxi_zone_lookup.csv`\n",
    "    ....\n",
    "```\n",
    "\n",
    "\n",
    "Before creating the pipeline, let's check the datasets for the yellow and green taxi from 01-2019 and for the taxi zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:46:40</td>\n",
       "      <td>2019-01-01 00:53:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>151</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:59:47</td>\n",
       "      <td>2019-01-01 01:18:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2019-01-01 00:46:40   2019-01-01 00:53:20              1.0   \n",
       "1         1  2019-01-01 00:59:47   2019-01-01 01:18:59              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            1.5         1.0                  N           151           239   \n",
       "1            2.6         1.0                  N           239           246   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1          7.0    0.5      0.5        1.65           0.0   \n",
       "1             1         14.0    0.5      0.5        1.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge airport_fee  \n",
       "0                    0.3          9.95                   NaN        None  \n",
       "1                    0.3         16.30                   NaN        None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-21 15:17:29</td>\n",
       "      <td>2018-12-21 15:18:57</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:10:16</td>\n",
       "      <td>2019-01-01 00:16:32</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97</td>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0         2  2018-12-21 15:17:29   2018-12-21 15:18:57                  N   \n",
       "1         2  2019-01-01 00:10:16   2019-01-01 00:16:32                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0         1.0           264           264              5.0           0.00   \n",
       "1         1.0            97            49              2.0           0.86   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  ehail_fee  \\\n",
       "0          3.0    0.5      0.5         0.0           0.0        NaN   \n",
       "1          6.0    0.5      0.5         0.0           0.0        NaN   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \\\n",
       "0                    0.3           4.3           2.0        1.0   \n",
       "1                    0.3           7.3           2.0        1.0   \n",
       "\n",
       "   congestion_surcharge  \n",
       "0                   NaN  \n",
       "1                   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>borough</th>\n",
       "      <th>zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id borough            zone service_zone\n",
       "0            1     EWR  Newark Airport          EWR\n",
       "1            2  Queens     Jamaica Bay    Boro Zone"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yellow_tripdata_2019_01 = pd.read_parquet('data/yellow_tripdata_2019-01.parquet')\n",
    "green_tripdata_2019_01 = pd.read_parquet('data/green_tripdata_2019-01.parquet')\n",
    "taxi_zone_lookup = pd.read_csv('data/taxi_zone_lookup.csv')\n",
    "\n",
    "display(yellow_tripdata_2019_01.head(2))\n",
    "display(green_tripdata_2019_01.head(2))\n",
    "display(taxi_zone_lookup.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In airflow a pipeline is called a Directed Acyclic Graph (DAG). A DAG is a collection of all the tasks we want to run, organized in a way that reflects their relationships and dependencies. The main idea for this DAG is to ingest the Green and Yellow taxi data from the [NYC taxi](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) into a bucket in Google Cloud Storage and then create a external table in BigQuery. The following diagram shows the final DAG structure that we desire to construct:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/dag-black.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "Let's describe each task in the DAG structure:\n",
    "\n",
    "1. **download_data**: The DAG begins by downloading the latest green and yellow taxi trip data with the `BashOperator`, formatted as parquet files, from specified URLs. \n",
    "\n",
    "2. **transform_green_taxi_columns_to_snake** and **transform_yellow_taxi_columns_to_snake**: The `PythonOperator` task will transform the column names of the green taxi trip data to snake case.\n",
    "\n",
    "3. **create_bucket**: create a bucket in Google Cloud Storage to store the taxi data with the `GCSCreateBucketOperator`.\n",
    "\n",
    "4. **ingest_green_taxi** and **ingest_yellow_taxi**: The `PythonOperator` will ingest the green and yellow taxi trip data into a bucket in Google Cloud Storage.\n",
    "\n",
    "5. **create_empty_dataset**: The `BigQueryCreateEmptyDatasetOperator` will create a empty dataset in BigQuery to store the taxi tables.\n",
    "\n",
    "6. **create_green_taxi_table** and **create_yellow_taxi_table**: The `BigQueryCreateExternalTableOperator` will create a external table in BigQuery to store the green and yellow taxi trip data.\n",
    "\n",
    "For more information about each operator, we can check the [Astronomer website](https://registry.astronomer.io/). In the search bar we can type the name of the operator and check the documentation and some examples of how to use it.\n",
    "\n",
    "To create this DAG, first create a `.py` file named `elt_nyc_taxi_bq.py` inside the `dags` directory. The content of the `elt_nyc_taxi_bq.py` file would be:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size =5 color ='orange'> \n",
    "elt_nyc_taxi_bq.py\n",
    "</font>\n",
    "\n",
    "```python\n",
    "    # [START import modules] \n",
    "    from airflow import DAG  \n",
    "    from datetime import datetime  \n",
    "    from google.cloud import storage  # For accessing Google Cloud Storage\n",
    "    import pandas as pd  \n",
    "    import re  \n",
    "    import pyarrow.parquet as pq  \n",
    "    import pyarrow as pa  \n",
    "    from os import getenv  \n",
    "\n",
    "    # Import specific operators from Airflow\n",
    "    from airflow.operators.bash import BashOperator\n",
    "    from airflow.operators.python import PythonOperator\n",
    "    from airflow.providers.google.cloud.operators.gcs import GCSCreateBucketOperator\n",
    "    from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\n",
    "    from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateExternalTableOperator, BigQueryCreateEmptyDatasetOperator\n",
    "    # [END import modules] \n",
    "\n",
    "    # [START Env Variables]----------------------------------------------------------------\n",
    "    # Define environment variables for file paths and GCP configurations\n",
    "    # These variables allow for dynamic data paths and project settings\n",
    "    BASE_URL_1 = 'https://d37ci6vzurychx.cloudfront.net/trip-data'  \n",
    "    BASE_URL_2 = 'https://d37ci6vzurychx.cloudfront.net/trip-data' \n",
    "    FILE_NAME_1 = 'green_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet'  \n",
    "    FILE_NAME_2 = 'yellow_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet'  \n",
    "\n",
    "    # Complete URLs for taxi data\n",
    "    URL_1 = f'{BASE_URL_1}/{FILE_NAME_1}'  \n",
    "    URL_2 = f'{BASE_URL_2}/{FILE_NAME_2}' \n",
    "\n",
    "     # Airflow home directory\n",
    "    AIRFLOW_HOME = getenv(\"AIRFLOW_HOME\", \"/usr/local/airflow\")          \n",
    "    # Paths to save taxi data\n",
    "    FILE_PATH_1 = getenv('FILE_PATH_1', f'{AIRFLOW_HOME}/{FILE_NAME_1}') \n",
    "    FILE_PATH_2 = getenv('FILE_PATH_2', f'{AIRFLOW_HOME}/{FILE_NAME_2}')  \n",
    "\n",
    "    DATASET_NAME = getenv(\"DATASET_NAME\", 'nyc_taxi')                   # BigQuery dataset name\n",
    "    TABLE_NAME = 'green_taxi_{{ execution_date.strftime(\\'%Y_%m\\') }}'  # Table name pattern\n",
    "\n",
    "    # Year to donwload and table if for taxi data\n",
    "    YEAR = 2019\n",
    "    TABLE_ID_1 = f\"green_taxi_external_{YEAR}\"\n",
    "    TABLE_ID_2 = f\"yellow_taxi_external_{YEAR}\"\n",
    "\n",
    "    PROJECT_ID = getenv(\"PROJECT_ID\", \"de-bootcamp-414215\")     # GCP Project ID\n",
    "    REGION = getenv(\"REGIONAL\", \"us-east1\")                     \n",
    "    LOCATION = getenv(\"LOCATION\", \"us-east1\")                    \n",
    "\n",
    "    BUCKET_NAME = getenv(\"BUCKET_NAME\", 'nyc-taxi-data-414215')\n",
    "    # GCS folder for storing taxi data inside the bucket     \n",
    "    GCS_BUCKET_FOLDER = getenv(\"GCS_BUCKET\", 'nyc_taxi_trip_2019') \n",
    "\n",
    "    # Connection ID created in Airflow UI\n",
    "    CONNECTION_ID = getenv(\"CONNECTION_ID\", \"gcp_conn\") \n",
    "    # [END Env Variables]\n",
    "\n",
    "    # [START default args] ----------------------------------------------------------------\n",
    "    # Define default arguments for the DAG\n",
    "    default_args = {\n",
    "        \"owner\": \"marcos benicio\",\n",
    "        \"email\": ['marcosbenicio@id.uff.br'],\n",
    "        \"email_on_failure\": False,\n",
    "        \"email_on_retry\": False,\n",
    "        \"retries\": 1  \n",
    "    }\n",
    "    # [END default args]\n",
    "\n",
    "    # [START Python Functions]----------------------------------------------------------------\n",
    "    # Define custom Python functions for data transformation and uploading\n",
    "    def transform_columns_to_snake(file_path):\n",
    "        \"\"\"\n",
    "        Transforms column names from camel case to snake case for consistency.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The file path of the parquet file to transform.\n",
    "        \"\"\"\n",
    "        original_table = pq.read_table(file_path)       \n",
    "        original_column_names = original_table.schema.names  \n",
    "        original_metadata = original_table.schema.metadata \n",
    "\n",
    "        # Function to convert camel case to snake case\n",
    "        def camel_to_snake(name):\n",
    "            return re.sub(r'(?<=[a-z0-9])([A-Z])|(?<=[A-Z])([A-Z])(?=[a-z])', r'_\\g<0>', name).lower()\n",
    "\n",
    "         # Transform column names\n",
    "        new_column_names = [camel_to_snake(name) for name in original_column_names] \n",
    "        # Create new fields with transformed names and use to create a new schema and table\n",
    "        fields = [pa.field(new_name, original_table.schema.field(original_name).type) \n",
    "                  for new_name, original_name in zip(new_column_names, original_column_names)]\n",
    "\n",
    "        new_schema = pa.schema(fields, metadata=original_metadata)  \n",
    "        new_table = pa.Table.from_arrays(original_table.columns, schema=new_schema)  \n",
    "\n",
    "        # Overwrite the transformed table back to the file\n",
    "        pq.write_table(new_table, file_path)\n",
    "\n",
    "    def filesystem_to_gcs(bucket, dst, src):\n",
    "        \"\"\"\n",
    "        Uploads a file from the local filesystem to Google Cloud Storage, \n",
    "        adjusting settings to prevent timeouts on large files.\n",
    "\n",
    "        Args:\n",
    "            bucket (str): The name of the GCS bucket.\n",
    "            dst (str): The destination path and file name within the GCS bucket.\n",
    "            src (str): The source path and file name on the local filesystem.\n",
    "        \"\"\"\n",
    "        # Set max multipart upload size to 5 MB\n",
    "        storage.blob._MAX_MULTIPART_SIZE = 5 * 1024 * 1024  \n",
    "        # Set default chunk size to 5 MB to prevent timeouts\n",
    "        storage.blob._DEFAULT_CHUNKSIZE = 5 * 1024 * 1024 \n",
    "\n",
    "        # Initialize the GCS client and get bucket object\n",
    "        client = storage.Client()  \n",
    "        bucket = client.bucket(bucket)  \n",
    "\n",
    "        # Create a blob object with the destination path and upload file\n",
    "        blob = bucket.blob(dst)  \n",
    "        blob.upload_from_filename(src) \n",
    "\n",
    "        print(f\"File {src} uploaded to {dst} in bucket {bucket}.\")\n",
    "    # [END Python Functions]\n",
    "\n",
    "    # [START DAG Object]---------------------------------------------------------------- \n",
    "    # Initialize the DAG object\n",
    "    workflow = DAG(\n",
    "            dag_id=\"elt_nyc_taxi_bq\",\n",
    "            default_args = default_args,\n",
    "            description=\"\"\"A DAG to export data from NYC taxi web, \n",
    "            load the taxi trip data into GCS to create a BigQuery external table \n",
    "            and transform the data with Dbt\"\"\",\n",
    "            tags=['gcs', 'bigquery','data_elt', 'dbt', 'nyc_taxi'], \n",
    "            schedule_interval=\"0 6 28 * *\",\n",
    "            start_date = datetime(YEAR, 1, 1),\n",
    "            end_date = datetime(YEAR, 12, 30)\n",
    "                )\n",
    "    # [END DAG Object]\n",
    "\n",
    "    # [START Workflow]---------------------------------------------------------------- \n",
    "    # Define the workflow using the DAG object\n",
    "    with workflow:\n",
    "        # Download taxi data from source URLs\n",
    "        download_data = BashOperator(\n",
    "            task_id=\"download_data\",\n",
    "            bash_command=f\"\"\"    \n",
    "                            curl -sSLo {FILE_PATH_1} {URL_1} && \\\\\n",
    "                            curl -sSLo {FILE_PATH_2} {URL_2}\n",
    "                            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Transform column names of the green taxi data to snake case\n",
    "        transform_green_taxi_columns_to_snake = PythonOperator(\n",
    "            task_id='transform_green_taxi_columns_to_snake',\n",
    "            python_callable=transform_columns_to_snake,\n",
    "            op_kwargs={'file_path': FILE_PATH_1},\n",
    "        )\n",
    "\n",
    "        # Transform column names of the yellow taxi data to snake case\n",
    "        transform_yellow_taxi_columns_to_snake = PythonOperator(\n",
    "            task_id='transform_yellow_taxi_columns_to_snake',\n",
    "            python_callable=transform_columns_to_snake,\n",
    "            op_kwargs={'file_path': FILE_PATH_2},\n",
    "        )\n",
    "\n",
    "        # Create a GCS bucket if it doesn't exist\n",
    "        create_bucket = GCSCreateBucketOperator(\n",
    "            task_id=\"create_bucket\",\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            storage_class=\"REGIONAL\",\n",
    "            location=LOCATION,\n",
    "            project_id=PROJECT_ID,\n",
    "            labels={\"env\": \"dev\", \"team\": \"airflow\"},\n",
    "            gcp_conn_id=CONNECTION_ID\n",
    "        )\n",
    "\n",
    "        # Upload the transformed green taxi data to GCS\n",
    "        ingest_green_taxi_gcs = PythonOperator(\n",
    "            task_id=\"ingest_green_taxi\",\n",
    "            python_callable=filesystem_to_gcs,\n",
    "            op_kwargs={\"bucket\": BUCKET_NAME, \"dst\": f\"{GCS_BUCKET_FOLDER}/{FILE_NAME_1}\", \"src\": FILE_PATH_1}\n",
    "        )\n",
    "\n",
    "        # Upload the transformed yellow taxi data to GCS\n",
    "        ingest_yellow_taxi_gcs = PythonOperator(\n",
    "            task_id=\"ingest_yellow_taxi\",\n",
    "            python_callable=filesystem_to_gcs,\n",
    "            op_kwargs={\"bucket\": BUCKET_NAME, \"dst\": f\"{GCS_BUCKET_FOLDER}/{FILE_NAME_2}\", \"src\": FILE_PATH_2}\n",
    "        )\n",
    "\n",
    "        # Create an empty dataset in BigQuery if it doesn't exist\n",
    "        create_empty_dataset = BigQueryCreateEmptyDatasetOperator(\n",
    "            task_id=\"create_empty_dataset\",\n",
    "            dataset_id=DATASET_NAME,\n",
    "            project_id=PROJECT_ID,\n",
    "            location=LOCATION,\n",
    "            gcp_conn_id=CONNECTION_ID\n",
    "        )\n",
    "        # Create an external table in BigQuery for the green taxi data\n",
    "        bigquery_green_taxi_table = BigQueryCreateExternalTableOperator(\n",
    "            task_id=\"create_green_taxi_table\",\n",
    "            table_resource={\n",
    "                'tableReference': {\n",
    "                    'projectId': PROJECT_ID,\n",
    "                    'datasetId': DATASET_NAME,\n",
    "                    'tableId': TABLE_ID_1,\n",
    "                },\n",
    "                'externalDataConfiguration': {\n",
    "                    'sourceFormat': 'PARQUET',\n",
    "                    'sourceUris': [f\"gs://{BUCKET_NAME}/{GCS_BUCKET_FOLDER}/green_tripdata_*.parquet\"],\n",
    "                }\n",
    "            },\n",
    "            gcp_conn_id=CONNECTION_ID\n",
    "        )\n",
    "        # Create an external table in BigQuery for the green taxi data\n",
    "        bigquery_yellow_taxi_table = BigQueryCreateExternalTableOperator(\n",
    "            task_id=\"create_yellow_taxi_table\",\n",
    "            table_resource={\n",
    "                'tableReference': {\n",
    "                    'projectId': PROJECT_ID,\n",
    "                    'datasetId': DATASET_NAME,\n",
    "                    'tableId': TABLE_ID_2,\n",
    "                },\n",
    "                'externalDataConfiguration': {\n",
    "                    'sourceFormat': 'PARQUET',\n",
    "                    'sourceUris': [f\"gs://{BUCKET_NAME}/{GCS_BUCKET_FOLDER}/yellow_tripdata_*.parquet\"],\n",
    "                }\n",
    "            },\n",
    "            gcp_conn_id=CONNECTION_ID\n",
    "        )\n",
    "\n",
    "    download_data >> [transform_green_taxi_columns_to_snake, transform_yellow_taxi_columns_to_snake] \\\n",
    "    >> create_bucket >> [ingest_green_taxi_gcs, ingest_yellow_taxi_gcs ] \\\n",
    "    >> create_empty_dataset >> [bigquery_yellow_taxi_table, bigquery_green_taxi_table]\n",
    "    # [END Workflow] \n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `elt_nyc_taxi_bq.py` inside the `dags` directory, we can now access the Airflow UI and check if the DAG is available and run it. In the Airflow UI we should see the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/airflow-dag.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "Also, don't forget to configure the connections in the Airflow UI. The connections are the way to connect the Airflow with the Google Cloud Platform. To set a connection go to Admin -> Connections and click on the `Create` button and select the `Google Cloud Platform` option. This will require again a json file with the credentials to authenticate the Airflow with the Google Cloud Platform, the same used before.\n",
    "\n",
    "We are now with the following directory structure:\n",
    "\n",
    "```markdown\n",
    "astro-airflow/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── taxi_rides_ny/\n",
    "    │   │   ├── `elt_nyc_taxi_bq.py`\n",
    "    ├── include/\n",
    "    ├── plugins/\n",
    "    ├── tests/\n",
    "    ├── .dockerignore\n",
    "    ├── .env                    \n",
    "    ├── .gitignore\n",
    "    ├── airflow_settings.yaml  \n",
    "    ├── dbt-requirements.txt \n",
    "    ├── Dockerfile\n",
    "    ├── docker-compose.override.yml\n",
    "    ├── packages.txt               \n",
    "    ├── requirements.txt     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 dbt Macros and Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dbt, macros are pieces of code written in Jinja that are used for generating SQL queries. They are essentially functions that can be defined to encapsulate logic using `if` and `for` statements within SQL code for reuse across a dbt project. Macros can be used to perform operations like data manipulation, formatting, and conditional logic, making  dbt models more dynamic and modular. They help to keep the code DRY (Don't Repeat Yourself) by allowing to write a piece of logic once and reuse it in multiple models or analyses.\n",
    "\n",
    "Let's create some macros that will be used in our project. To create a macro, we need to create a file with the `.sql` extension inside the `dags/dbt/taxi_rides_ny/macros` directory. The content of the `get_payment_type_description.sql` file would be:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "get_payment_type_description.sql\n",
    "</font>\n",
    "\n",
    "```sql\n",
    "    {#\n",
    "        This macro returns the description of the payment_type \n",
    "    #}\n",
    "\n",
    "    {% macro get_payment_type_description(payment_type) -%}\n",
    "\n",
    "        case ( {{ payment_type }} as integer)  \n",
    "            when 1 then 'Credit card'\n",
    "            when 2 then 'Cash'\n",
    "            when 3 then 'No charge'\n",
    "            when 4 then 'Dispute'\n",
    "            when 5 then 'Unknown'\n",
    "            when 6 then 'Voided trip'\n",
    "            else 'EMPTY'\n",
    "        end\n",
    "\n",
    "    {%- endmacro %}\n",
    "``` \n",
    "\n",
    "This macro is designed to return the description of a payment type based on its numerical value. `get_payment_type_description` is the function name, and `payment_type` is the parameter that will be passed to the function. The `payment_type` parameter is used to determine the payment type description.\n",
    "\n",
    "The `{%-` indicates to Jinja to strip any whitespace that appears immediately after the tag. It ensures that there is no whitespace before the end of the macro, which can be important when the macro is used in generating code or queries, where extra whitespace could cause syntax errors or unintended formatting. The same idea is used for `{%-` at the end, ensuring that any whitespace immediately before the macro tag is removed.\n",
    "\n",
    "We can also import packages from [dbt package hub](https://hub.getdbt.com/) like libraries in other programming languages. By adding packages to the `packages.yml` file, we can use the functions and macros defined in the packages in our dbt project. Let's create the `packages.yml` file inside the `dags/dbt/taxi_rides_ny` directory and add the [`dbt_utils`](https://hub.getdbt.com/dbt-labs/dbt_utils/latest/)  and the [`codegen`](https://hub.getdbt.com/dbt-labs/codegen/latest/) packages to it.\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "packages.yml\n",
    "</font>\n",
    "\n",
    "```yaml\n",
    "    packages:\n",
    "      - package: dbt-labs/dbt_utils\n",
    "        version: 1.1.1\n",
    "      - package: dbt-labs/codegen\n",
    "        version: 0.12.1\n",
    "```\n",
    "To instal the packages, we need to run the following command inside the `dags/dbt/taxi_rides_ny` directory, where the `dbt_project.yml` is located directory:\n",
    "\n",
    "```bash\n",
    "    dbt deps\n",
    "```\n",
    "\n",
    "We should see the new directory `dbt_packages` inside the `dags/dbt/taxi_rides_ny` directory. This directory contains the packages that was defined in the `packages.yml` file. The final directory structure would be:\n",
    "\n",
    "```markdown\n",
    "astro-airflow/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── taxi_rides_ny/\n",
    "    │   │   │   ├── analyses/\n",
    "    │   │   │   ├── `dbt_packages/`   \n",
    "    │   │   │   ├── models/\n",
    "    │   │   │   ├── macros/\n",
    "    │   │   │   │   ├── `get_payment_type_description.sql`\n",
    "    │   │   │   ├── seeds/\n",
    "    │   │   │   ├── snapshots/\n",
    "    │   │   │   ├── tests/\n",
    "    │   │   │   ├── dbt_project.yml\n",
    "    │   │   │   ├── `packages.yml`\n",
    "    │   │   │   ├── `packages.lock.yml`    \n",
    "        ...     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Staging Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staging models are typically the first step in transforming raw data into a format that's more suitable for analysis. These models are crucial for ensuring consistency, cleanliness, and reliability of the data. Let's create inside `dags/dbt/taxi_rides_ny/model/` the `staging` directory with a `schema.yml`,  `stg_staging_green_tripdata.sql` ,`stg_staging_yellow_tripdata.sql`. The directory structure would be:\n",
    "\n",
    "\n",
    "```markdown\n",
    "astro-airflow/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── taxi_rides_ny/\n",
    "    │   │   │   ├── analyses/\n",
    "    │   │   │   ├── dbt_packages/  \n",
    "    │   │   │   ├── models/\n",
    "    │   │   │   │    ├── staging/\n",
    "    │   │   │   │    │    ├── `schema.yml`\n",
    "    │   │   │   │    │    ├── `stg_green_tripdata.sql`\n",
    "    │   │   │   │    │    ├── `stg_yellow_tripdata.sql`    \n",
    "    │   │   │   ├── macros/\n",
    "    │   │   │   ├── seeds/\n",
    "    │   │   │   ├── snapshots/\n",
    "    │   │   │   ├── tests/\n",
    "    │   │   │   ├── dbt_project.yml\n",
    "    │   │   │   ├── packages.yml\n",
    "    │   │   │   ├── packages.lock.yml\n",
    "        ...\n",
    "```\n",
    "\n",
    "Starting by the `schema.yml`, we configure as follows for the tables that we previously ingested into the BigQuery:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "schema.yml\n",
    "</font>\n",
    "\n",
    "  ```yaml\n",
    "    version: 2\n",
    "\n",
    "    sources:\n",
    "      - name: staging                           # directory name in dbt/models\n",
    "        database: de-bootcamp-414215            # dataset name in BigQuery\n",
    "        schema: nyc_taxi                        # schema name in BigQuery where the table is located\n",
    "        tables:                                 # list of tables in the schema\n",
    "          - name: \"green_taxi_external_2019\"\n",
    "          - name: \"yellow_taxi_external_2019\"\n",
    "  ```\n",
    "\n",
    "Next, in the `stg_staging_green_tripdata.sql` create the code using sql and jinja to transforms raw data from the `green_taxi_external_2019` table in BigQuery, as our source, into a view with properly formatted and casted columns, including a description of the payment type, and applies filtering to select only the first row for each combination of `vendor_id` and `lpep_pickup_datetime`. In BigQuery we should be able to see the `nyc_taxi` schema with the tables `green_taxi_external_2019` and `yellow_taxi_external_2019` as external tables as follows:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/bigquery-green-schema-table.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "stg_staging_green_tripdata.sql\n",
    "</font>\n",
    "\n",
    "  ```sql\n",
    "    {{  config( materialized='view')  }}\n",
    "        \n",
    "    WITH tripdata AS \n",
    "    (\n",
    "        SELECT *,\n",
    "        row_number() OVER(PARTITION BY vendor_id, lpep_pickup_datetime) AS rn\n",
    "        FROM {{ source('staging','green_taxi_external_2019') }}\n",
    "        WHERE vendor_id IS NOT NULL \n",
    "    )\n",
    "    SELECT\n",
    "        -- identifiers\n",
    "        {{ dbt_utils.generate_surrogate_key(['vendor_id', 'lpep_pickup_datetime']) }} AS trip_id,\n",
    "        {{ dbt.safe_cast(\"vendor_id\", api.Column.translate_type(\"integer\")) }} AS vendor_id,\n",
    "        {{ dbt.safe_cast(\"ratecode_id\", api.Column.translate_type(\"integer\")) }} AS ratecode_id,\n",
    "        {{ dbt.safe_cast(\"pu_location_id\", api.Column.translate_type(\"integer\")) }} AS pickup_location_id,\n",
    "        {{ dbt.safe_cast(\"do_location_id\", api.Column.translate_type(\"integer\")) }} AS dropoff_location_id,\n",
    "        \n",
    "        -- timestamps\n",
    "        cast(lpep_pickup_datetime AS timestamp) AS pickup_datetime,\n",
    "        cast(lpep_dropoff_datetime AS timestamp) AS dropoff_datetime,\n",
    "        \n",
    "        -- trip info\n",
    "        store_and_fwd_flag,\n",
    "        {{ dbt.safe_cast(\"passenger_count\", api.Column.translate_type(\"integer\")) }} AS passenger_count,\n",
    "        cast(trip_distance AS numeric) AS trip_distance,\n",
    "        {{ dbt.safe_cast(\"trip_type\", api.Column.translate_type(\"integer\")) }} AS trip_type,\n",
    "    \n",
    "        -- payment info\n",
    "        cast(fare_amount AS numeric) AS fare_amount,\n",
    "        cast(extra AS numeric) AS extra,\n",
    "        cast(mta_tax AS numeric) AS mta_tax,\n",
    "        cast(tip_amount AS numeric) AS tip_amount,\n",
    "        cast(tolls_amount AS numeric) AS tolls_amount,\n",
    "\n",
    "        cast(improvement_surcharge AS numeric) AS improvement_surcharge,\n",
    "        cast(total_amount AS numeric) AS total_amount,\n",
    "        coalesce({{ dbt.safe_cast(\"payment_type\", api.Column.translate_type(\"integer\")) }},0) AS payment_type,\n",
    "        {{ get_payment_type_description(\"payment_type\") }} AS payment_type_description\n",
    "    FROM tripdata\n",
    "    WHERE rn = 1\n",
    "    \n",
    "    \n",
    "    -- dbt build --select <model_name> --vars '{'is_test_run': 'false'}'\n",
    "    {% if var('is_test_run', default=true) %}\n",
    "    \n",
    "        LIMIT 100\n",
    "    \n",
    "    {% endif %}\n",
    "  ```\n",
    "---\n",
    "\n",
    "Let's better understand this code dissecting it into parts:\n",
    "\n",
    "**Part 1 - Configuration**\n",
    "\n",
    "The first piece of code is a configuration block used within a dbt model to specify the materialization type of the model.\n",
    "\n",
    "  ```sql\n",
    "    {{  config( materialized='view')  }}\n",
    "  ``` \n",
    "\n",
    "The `view` materialization is used to create a view in the database, which is a virtual table that does not store data, but instead retrieves data from the underlying tables when queried. Views provide a convenient way to represent and query data subsets or transformations without duplicating the underlying data.\n",
    "\n",
    "**Part 2 - Common Table Expression**\n",
    "\n",
    "The second piece of code is a common table expression (CTE) in SQL that is used to create a temporary result set that can be referenced within the main query. The `tripdata` CTE is used to create a temporary result set that contains the raw data from the `green_taxi_external_2019` source in BigQuery, as well as a row number for each combination of `vendor_id` and `lpep_pickup_datetime`.\n",
    "\n",
    "  ```sql\n",
    "    WITH tripdata AS \n",
    "    (\n",
    "      SELECT *,\n",
    "        ROW_NUMBER() OVER(PARTITION BY vendor_id, lpep_pickup_datetime) AS rn\n",
    "      FROM {{ source('staging','green_taxi_external_2019') }}\n",
    "      WHERE vendor_id IS NOT NULL \n",
    "    )\n",
    "  ```\n",
    "\n",
    "The `ROW_NUMBER()` function assigns a unique sequential integer to each row within a partition. In this query, it's used to generate a row number (`rn`) for each row within each partition defined by the combination of `vendor_id` and `lpep_pickup_datetime`. The `PARTITION BY` clause partitions the result set into groups based on the specified columns (`vendor_id` and `lpep_pickup_datetime`). For each distinct combination of `vendor_id` and `lpep_pickup_datetime`, the row numbers will start from 1 and increment for each subsequent row.\n",
    "\n",
    "**Part 3 - Main Query**\n",
    "\n",
    "The third piece of code is the main query that transforms the raw data from the `green_taxi_external_2019` staging source into a view with properly formatted and casted columns, including a description of the payment type, and applies filtering to select only the first row for each combination of `vendor_id` and `lpep_pickup_datetime`.\n",
    "  \n",
    "  ```sql\n",
    "    SELECT\n",
    "        -- identifiers\n",
    "        {{ dbt_utils.generate_surrogate_key(['vendor_id', 'lpep_pickup_datetime']) }} AS trip_id,\n",
    "        {{ dbt.safe_cast(\"vendor_id\", api.Column.translate_type(\"integer\")) }} AS vendor_id,\n",
    "        {{ dbt.safe_cast(\"ratecode_id\", api.Column.translate_type(\"integer\")) }} AS ratecode_id,\n",
    "        {{ dbt.safe_cast(\"pu_location_id\", api.Column.translate_type(\"integer\")) }} AS pickup_location_id,\n",
    "        {{ dbt.safe_cast(\"do_location_id\", api.Column.translate_type(\"integer\")) }} AS dropoff_location_id,\n",
    "        \n",
    "        -- timestamps\n",
    "        CAST(lpep_pickup_datetime AS TIMESTAMP) AS pickup_datetime,\n",
    "        CAST(lpep_dropoff_datetime AS TIMESTAMP) AS dropoff_datetime,\n",
    "        \n",
    "        -- trip info\n",
    "        store_and_fwd_flag,\n",
    "        {{ dbt.safe_cast(\"passenger_count\", api.Column.translate_type(\"integer\")) }} AS passenger_count,\n",
    "        CAST(trip_distance AS NUMERIC) AS trip_distance,\n",
    "        {{ dbt.safe_cast(\"trip_type\", api.Column.translate_type(\"integer\")) }} AS trip_type,\n",
    "    \n",
    "        -- payment info\n",
    "        CAST(fare_amount AS NUMERIC) AS fare_amount,\n",
    "        CAST(extra AS NUMERIC) AS extra,\n",
    "        CAST(mta_tax AS NUMERIC) AS mta_tax,\n",
    "        CAST(tip_amount AS NUMERIC) AS tip_amount,\n",
    "        CAST(tolls_amount AS NUMERIC) AS tolls_amount,\n",
    "        CAST(improvement_surcharge AS NUMERIC) AS improvement_surcharge,\n",
    "        CAST(total_amount AS NUMERIC) AS total_amount,\n",
    "        COALESCE({{ dbt.safe_cast(\"payment_type\", api.Column.translate_type(\"integer\")) }},0) AS payment_type,\n",
    "        {{ get_payment_type_description(\"payment_type\") }} AS payment_type_description\n",
    "    FROM tripdata\n",
    "    WHERE rn = 1\n",
    "  ```\n",
    "\n",
    "The `{{ dbt_utils.generate_surrogate_key(['vendor_id', 'lpep_pickup_datetime']) }}` function  from the package `dbt_utils` is used to generate a surrogate key for the `trip_id` column based on the combination of `vendor_id` and `lpep_pickup_datetime`. A surrogate key is a unique identifier for each row in a table that is not derived from the data itself, but rather generated by the system. It is often used as a primary key in a data warehouse to uniquely identify each row in a table. The `dbt.safe_cast()` and `api.translate_type()` are functions used to cast the specific column to a data type, where the `dbt` and `api` are packages pre built in dbt. At the end, the `get_payment_type_description` is the macro that we defined in the `get_payment_type_description.sql`. \n",
    "\n",
    "The `WHERE rn = 1` clause is used to filter the result set to select only the first row for each combination of `vendor_id` and `lpep_pickup_datetime`. Selecting only the first row for each combination can serve as a simple form of data sampling to analyze a representative subset of the dataset without processing the entire dataset.\n",
    "\n",
    "**Part 4 - Conditional Statement**\n",
    "\n",
    "The last piece of code is a conditional statement that limits the number of rows returned by the query when the `is_test_run` variable is set to `true`.\n",
    "\n",
    "  ```sql\n",
    "    {% if var('is_test_run', default=true) %}\n",
    "        \n",
    "      LIMIT 100\n",
    "\n",
    "    {% endif %}\n",
    "  ```\n",
    "This conditional statement is used to limit the number of rows returned by the query when running dbt in test mode, which can be useful for testing and debugging purposes. The `default=true` argument specifies that the `is_test_run` variable defaults to `true` if it is not explicitly set when running dbt.\n",
    "\n",
    "\n",
    "Following the same logic, we can do the same for the `stg_yellow_tripdata.sql` file. The schema in Bigquery should be as follows:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/bigquery-yellow-schema-table.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The content of the `stg_yellow_tripdata.sql` file would be:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "stg_staging_yellow_tripdata.sql\n",
    "</font>\n",
    "\n",
    "  ```sql\n",
    "    {{ config(materialized='view') }}\n",
    "    \n",
    "    WITH tripdata AS \n",
    "    (\n",
    "      SELECT *,\n",
    "        row_number() OVER(PARTITION BY vendor_id, tpep_pickup_datetime) AS rn\n",
    "      FROM {{ source('staging','yellow_taxi_external_2019') }}\n",
    "      WHERE vendor_id IS NOT NULL \n",
    "    )\n",
    "    SELECT\n",
    "      -- identifiers\n",
    "        {{ dbt_utils.generate_surrogate_key(['vendor_id', 'tpep_pickup_datetime']) }} AS trip_id,    \n",
    "        {{ dbt.safe_cast(\"vendor_id\", api.Column.translate_type(\"integer\")) }} AS vendor_id,\n",
    "        {{ dbt.safe_cast(\"ratecode_id\", api.Column.translate_type(\"integer\")) }} AS ratecode_id,\n",
    "        {{ dbt.safe_cast(\"pu_location_id\", api.Column.translate_type(\"integer\")) }} AS pickup_location_id,\n",
    "        {{ dbt.safe_cast(\"do_location_id\", api.Column.translate_type(\"integer\")) }} AS dropoff_location_id,\n",
    "\n",
    "        -- timestamps\n",
    "        CAST(tpep_pickup_datetime AS timestamp) AS pickup_datetime,\n",
    "        CAST(tpep_dropoff_datetime AS timestamp) AS dropoff_datetime,\n",
    "\n",
    "        -- trip info\n",
    "        store_and_fwd_flag,\n",
    "        {{ dbt.safe_cast(\"passenger_count\", api.Column.translate_type(\"integer\")) }} AS passenger_count,\n",
    "        CAST(trip_distance AS numeric) AS trip_distance,\n",
    "        -- yellow cabs are always street-hail\n",
    "        1 AS trip_type,\n",
    "\n",
    "        -- payment info\n",
    "        CAST(fare_amount AS numeric) AS fare_amount,\n",
    "        CAST(extra AS numeric) AS extra,\n",
    "        CAST(mta_tax AS numeric) AS mta_tax,\n",
    "        CAST(tip_amount AS numeric) AS tip_amount,\n",
    "        CAST(tolls_amount AS numeric) AS tolls_amount,\n",
    "        CAST(improvement_surcharge AS numeric) AS improvement_surcharge,\n",
    "        CAST(total_amount AS numeric) AS total_amount,\n",
    "        COALESCE({{ dbt.safe_cast(\"payment_type\", api.Column.translate_type(\"integer\")) }},0) AS payment_type,\n",
    "        {{ get_payment_type_description('payment_type') }} AS payment_type_description\n",
    "    FROM tripdata\n",
    "    WHERE rn = 1\n",
    "\n",
    "    -- dbt build --select <model.sql> --vars '{'is_test_run: false}'\n",
    "    {% if var('is_test_run', default=true) %}\n",
    "\n",
    "      LIMIT 100\n",
    "\n",
    "    {% endif %}\n",
    "  ```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3 Core**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Core folder within the Models directory of a dbt project is intended for storing models that perform more complex transformations on data that has already been pre-processed in the Staging models. These Core models typically include:\n",
    "\n",
    "- **Fact Tables**: These are the tables that contain the measures and metrics that will be analyze. Fact tables usually result from joining various staging models together and aggregating data to a level suitable for analysis.\n",
    "\n",
    "- **Dimension Tables**: Dimension tables are used to store descriptive attributes or dimensions through which fact data can be analyzed. They provide context to the numerical metrics stored in fact tables. Each dimension table contains a set of attributes (columns) that describe aspects of the business process represented in a fact table\n",
    "\n",
    "For example, a fact table might record a sale with a numeric sale amount and foreign keys linking to dimension tables; the dimension tables would then describe the product sold, the customer who bought it, and the date of the sale.\n",
    "\n",
    "Let's create inside `dags/dbt/taxi_rides_ny/model/` the `core` directory the  `fact_trips.sql` and `dim_zones.sql` files. The directory structure would be:\n",
    "\n",
    "\n",
    "```markdown\n",
    "astro-airflow/\n",
    "    ├── .astro/\n",
    "    ├── dags/\n",
    "    │   ├── dbt/\n",
    "    │   │   ├── logs\n",
    "    │   │   ├── taxi_rides_ny/\n",
    "    │   │   │   ├── analyses/\n",
    "    │   │   │   ├── dbt_packages/  \n",
    "    │   │   │   ├── models/\n",
    "    │   │   │   │    ├── staging/\n",
    "    │   │   │   │    ├── core/ \n",
    "    │   │   │   │    │    ├── `fact_trips.sql`\n",
    "    │   │   │   │    │    ├── `dim_zones.sql`\n",
    "    │   │   │   ├── macros/\n",
    "    │   │   │   ├── seeds/\n",
    "    │   │   │   │    ├── `taxi_zone_lookup.csv`   \n",
    "    │   │   │   ├── snapshots/\n",
    "    │   │   │   ├── tests/\n",
    "    │   │   │   ├── dbt_project.yml\n",
    "    │   │   │   ├── packages.yml\n",
    "    │   │   │   ├── packages.lock.yml\n",
    "```\n",
    "\n",
    "For the `taxi_zone_lookup.csv` table, we can create the `dim_zones.sql` file. The content of the `dim_zones.sql` file would be:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "dim_zones.sql\n",
    "</font>\n",
    "\n",
    "```sql\n",
    "{{ config(materialized='table') }}\n",
    "SELECT \n",
    "    location_id, \n",
    "    borough, \n",
    "    zone, \n",
    "    REPLACE(service_zone,'Boro','Green') AS service_zone \n",
    "FROM {{ ref('taxi_zone_lookup') }}\n",
    "```\n",
    "\n",
    "The `{{ ref('taxi_zone_lookup') }}` function is used to reference the `taxi_zone_lookup.csv` table inside the `seeds` directory. Remember that the  `taxi_zone_lookup` seed data contains information about the taxi zones in New York City, including the location ID, borough, zone, and service zone. The `SELECT` statement is used to select the location ID, borough, zone, and service zone from the `taxi_zone_lookup` seed data, and the `REPLACE()` function is used to replace the word 'Boro' with 'Green' in the service zone column. The Green taxis, officially known as Boro Taxis, were introduced to provide street hail service to areas outside of the central districts served by yellow taxis, this is why we are replacing the word 'Boro' with 'Green' in the service zone column.\n",
    "\n",
    "\n",
    "For the `fact_trips.sql` file, the content would be:\n",
    "\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "fact_trips.sql\n",
    "</font>\n",
    "\n",
    "```sql\n",
    "    {{ config(materialized='table') }}\n",
    "\n",
    "    WITH green_tripdata AS (\n",
    "        SELECT *, \n",
    "            'Green' AS service_type\n",
    "        FROM {{ ref('stg_green_tripdata') }}\n",
    "    ), \n",
    "    yellow_tripdata AS (\n",
    "        SELECT *, \n",
    "            'Yellow' AS service_type\n",
    "        FROM {{ ref('stg_yellow_tripdata') }}\n",
    "    ), \n",
    "    trips_unioned AS (\n",
    "        SELECT * FROM green_tripdata\n",
    "        UNION ALL \n",
    "        SELECT * FROM yellow_tripdata\n",
    "    ), \n",
    "    dim_zones AS (\n",
    "        SELECT * FROM {{ ref('dim_zones') }}\n",
    "        WHERE borough != 'Unknown'\n",
    "    )\n",
    "    SELECT \n",
    "        trips_unioned.trip_id, \n",
    "        trips_unioned.vendor_id, \n",
    "        trips_unioned.service_type,\n",
    "        trips_unioned.ratecode_id, \n",
    "        trips_unioned.pickup_location_id, \n",
    "        pickup_zone.borough AS pickup_borough, \n",
    "        pickup_zone.zone AS pickup_zone, \n",
    "        trips_unioned.dropoff_location_id,\n",
    "        dropoff_zone.borough AS dropoff_borough, \n",
    "        dropoff_zone.zone AS dropoff_zone,  \n",
    "        trips_unioned.pickup_datetime, \n",
    "        trips_unioned.dropoff_datetime, \n",
    "        trips_unioned.store_and_fwd_flag, \n",
    "        trips_unioned.passenger_count, \n",
    "        trips_unioned.trip_distance, \n",
    "        trips_unioned.trip_type, \n",
    "        trips_unioned.fare_amount, \n",
    "        trips_unioned.extra, \n",
    "        trips_unioned.mta_tax, \n",
    "        trips_unioned.tip_amount, \n",
    "        trips_unioned.tolls_amount, \n",
    "        trips_unioned.improvement_surcharge, \n",
    "        trips_unioned.total_amount, \n",
    "        trips_unioned.payment_type, \n",
    "        trips_unioned.payment_type_description\n",
    "    FROM trips_unioned\n",
    "    INNER JOIN dim_zones AS pickup_zone\n",
    "    ON trips_unioned.pickup_location_id = pickup_zone.location_id\n",
    "    INNER JOIN dim_zones AS dropoff_zone\n",
    "    ON trips_unioned.dropoff_location_id = dropoff_zone.location_id\n",
    "```\n",
    "\n",
    "Let's better understand this code dissecting it into parts:\n",
    "\n",
    "**Part 1 - Configuration**\n",
    "\n",
    "The jinja macro now is used to materialize this model as a physical table in the database.\n",
    "\n",
    "  ```sql\n",
    "    {{ config(materialized='table') }}\n",
    "  ``` \n",
    "\n",
    "**Part 2 - Common Table Expression**\n",
    "\n",
    "This second part of the code create a CTE table for the `green_tripdata` and `yellow_tripdata` tables, and then union them together to create the `trip_unioned` table. At the last part of the code we create a CTE table for the `dim_zones` table for borough rows with values different from unknown.\n",
    "\n",
    "\n",
    "```sql\n",
    "    WITH green_tripdata AS (\n",
    "        SELECT *, \n",
    "            'Green' AS service_type\n",
    "        FROM {{ ref('stg_green_tripdata') }}\n",
    "    ), \n",
    "    yellow_tripdata AS (\n",
    "        SELECT *, \n",
    "            'Yellow' AS service_type\n",
    "        FROM {{ ref('stg_yellow_tripdata') }}\n",
    "    ), \n",
    "    trips_unioned AS (\n",
    "        SELECT * FROM green_tripdata\n",
    "        UNION ALL \n",
    "        SELECT * FROM yellow_tripdata\n",
    "    ), \n",
    "    dim_zones AS (\n",
    "        SELECT * FROM {{ ref('dim_zones') }}\n",
    "        WHERE borough != 'Unknown'\n",
    "    )\n",
    "```\n",
    "\n",
    "**Part 3 - Main Query**\n",
    "\n",
    "This part it selects various fields from the trips_unioned CTE, which contains combined data from both green and yellow taxi trips. The `dim_zones` table is joined with INNER JOIN to ensures that only records with matching location IDs in both `trips_unioned` and `dim_zones` are selected. This means the query will only return trip records that have a known pickup and dropoff location within the zones defined in the `dim_zones` table.\n",
    "\n",
    "```sql\n",
    "    SELECT \n",
    "        trips_unioned.trip_id, \n",
    "        trips_unioned.vendor_id, \n",
    "        trips_unioned.service_type,\n",
    "        trips_unioned.ratecode_id, \n",
    "        trips_unioned.pickup_location_id, \n",
    "        pickup_zone.borough AS pickup_borough, \n",
    "        pickup_zone.zone AS pickup_zone, \n",
    "        trips_unioned.dropoff_location_id,\n",
    "        dropoff_zone.borough AS dropoff_borough, \n",
    "        dropoff_zone.zone AS dropoff_zone,  \n",
    "        trips_unioned.pickup_datetime, \n",
    "        trips_unioned.dropoff_datetime, \n",
    "        trips_unioned.store_and_fwd_flag, \n",
    "        trips_unioned.passenger_count, \n",
    "        trips_unioned.trip_distance, \n",
    "        trips_unioned.trip_type, \n",
    "        trips_unioned.fare_amount, \n",
    "        trips_unioned.extra, \n",
    "        trips_unioned.mta_tax, \n",
    "        trips_unioned.tip_amount, \n",
    "        trips_unioned.tolls_amount, \n",
    "        trips_unioned.improvement_surcharge, \n",
    "        trips_unioned.total_amount, \n",
    "        trips_unioned.payment_type, \n",
    "        trips_unioned.payment_type_description\n",
    "    FROM trips_unioned\n",
    "    INNER JOIN dim_zones AS pickup_zone\n",
    "    ON trips_unioned.pickup_location_id = pickup_zone.location_id\n",
    "    INNER JOIN dim_zones AS dropoff_zone\n",
    "    ON trips_unioned.dropoff_location_id = dropoff_zone.location_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the last part of our dbt model, we can create the `dim_monthly_zone_revenue.sql` file. The content of the `dim_monthly_zone_revenue.sql` file would be:\n",
    "\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "dim_monthly_zone_revenue.sql\n",
    "</font>\n",
    "\n",
    "```sql\n",
    "    {{ config(materialized='table') }}\n",
    "\n",
    "    WITH trips_data AS (\n",
    "        SELECT * FROM {{ ref('fact_trips') }}\n",
    "    )\n",
    "        SELECT \n",
    "            -- Reveneue grouping \n",
    "            pickup_zone AS revenue_zone,\n",
    "            {{ dbt.date_trunc(\"month\", \"pickup_datetime\") }} AS revenue_month, \n",
    "            service_type, \n",
    "            -- Revenue calculation \n",
    "            SUM(fare_amount) AS revenue_monthly_fare,\n",
    "            SUM(extra) AS revenue_monthly_extra,\n",
    "            SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "            SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "            SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "            SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "            SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "\n",
    "            -- Additional calculations\n",
    "            COUNT(trip_id) AS total_monthly_trips,\n",
    "            AVG(passenger_count) AS avg_monthly_passenger_count,\n",
    "            AVG(trip_distance) AS avg_monthly_trip_distance\n",
    "        FROM trips_data\n",
    "        GROUP BY 1,2,3\n",
    "``` \n",
    "\n",
    "The `{{ dbt.date_trunc(\"month\", \"pickup_datetime\") }}` truncate the `pickup_datetime` column to the first day of each month, effectively grouping data by month, and to refer to this truncated date as revenue_month in the output of the query. The use of this macro is a cross database macro to abstract the underlying SQL flavour and provide a consistent interface for date truncation across different databases. \n",
    "\n",
    "The `SUM()` function is used to calculate the total revenue for each revenue category, and the `AVG()` function is used to calculate the average passenger count and trip distance for each month. The `COUNT()` function is used to calculate the total number of trips for each month. The `GROUP BY 1, 2, 3` is shorthand syntax in SQL that references the columns selected in the SELECT clause  to group the revenue data by `revenue_zone`, `revenue_month`, and `service_type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.5 Running dbt models in Airflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the dbt models in Airflow, we need to increment our `elt_nyc_taxi_bq.py` file with the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.6.  Testing and Documenting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test are assumptions that we make about our data, and they are used to ensure that the data is accurate, consistent, and reliable. We can add tests and descriptions for each column inside the `schema.yml` file. To easily generate the content of the `schema.yml` file for the staging and core models inside the directory `models` we can use the package [`dbt-labs/codegen`](https://hub.getdbt.com/dbt-labs/codegen/latest/) that was installed.\n",
    "\n",
    "We can call a macro operation in the bash to generate the schema.yml file for the models in the `staging` and `core` directories. Let's create a bash script to automatically update the `schema.yml` file for the models in the `staging` and `core` directories. The content of the `update_schema.sh` file would be:\n",
    "\n",
    "<font size =5 color ='orange'> \n",
    "update_schema.sh\n",
    "</font>\n",
    "\n",
    "```bash\n",
    "    # Run codegen for staging models\n",
    "    output_staging=$(dbt run-operation generate_schema_yml --args '{\"directory\": \"staging\", \"prefix\": \"stg_\"}')\n",
    "    echo \"$output_staging\" >> models/staging/schema.yml\n",
    "\n",
    "    # Run codegen for core models\n",
    "    output_core=$(dbt run-operation generate_schema_yml --args '{\"directory\": \"core\"}')\n",
    "    echo \"$output_core\" >> models/core/schema.yml\n",
    "```\n",
    "\n",
    "This will append the output of the `dbt run-operation generate_schema_yml` command to the `schema.yml` file for the staging and core models. Inside the directory `dags/dbt/taxi_rides_ny` run the following command to make the `update_schema.sh` file executable:\n",
    "    \n",
    "```bash\n",
    "    chmod +x update_schema.sh\n",
    "    ./update_schema.sh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Full DAG to Ingest and Transform Data in BigQuery**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
