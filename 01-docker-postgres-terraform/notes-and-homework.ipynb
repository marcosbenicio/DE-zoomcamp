{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outline**\n",
    "\n",
    "- [**1. Docker**](#1.-docker)\n",
    "    - [**1.1. Homework: Docker basics**](##1.1.-homework:-docker-basics)\n",
    "- [**2. Postgres**](#2.-postgres)\n",
    "    - [2.1. Running Postgres in Docker with pgcli](##2.1.-running-postgres-in-docker-with-pgcli)\n",
    "    - [2.2. Ingesting Data Into Postgres](##2.2.-ingesting-data-into-postgres)\n",
    "    - [2.3. Running Postgres and pgAdmin with Docker](##2.3.-running-postgres-and-pgadmin-with-docker)\n",
    "    - [2.4. Dockerizing the data ingestion](##2.4.-dockerizing-the-data-ingestion)\n",
    "    - [2.5. Running Postgres and pgAdmin with Docker-Compose](##2.5.-running-postgres-and-pgadmin-with-docker-compose)\n",
    "    - [2.6. Homework: using SQL](##2.6.-homework:-using-sql)\n",
    "- [**3. Terraform**](#3.-terraform)\n",
    "    - [3.1. Creating  and configuring a project on Google Cloud Platform (GCP)](##3.2.-creating--and-configuring-a-project-on-google-cloud-platform-(gcp))\n",
    "    - [3.2. Configure and Deploy with Terraform](##3.2.-configure-and-deploy-with-terraform)\n",
    "    - [3.3. Homework: Terraform](##3.3.-homework:-terraform)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Docker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Homework: Docker basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Question 1. Knowing docker tags**\n",
    "\n",
    "Run the command to get information on Docker \n",
    "\n",
    "```docker --help```\n",
    "\n",
    "Now run the command to get help on the \"docker build\" command:\n",
    "\n",
    "```docker build --help```\n",
    "\n",
    "Do the same for \"docker run\".\n",
    "\n",
    "Which tag has the following text? - *Automatically remove the container when it exits* \n",
    "\n",
    "- `--delete`\n",
    "- `--rc`\n",
    "- `--rmc`\n",
    "- **`--rm`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search for the command using pipe operator `|` and `grep` command to filter the output:\n",
    "\n",
    "```bash\n",
    "    docker run --help | grep \"Automatically\"\n",
    "```\n",
    "**output:**\n",
    "```bash\n",
    "    --rm                             Automatically remove the container when it exits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question 2. Understanding docker first run**\n",
    "\n",
    "Run docker with the python:3.9 image in an interactive mode and the entrypoint of bash.\n",
    "Now check the python modules that are installed ( use ```pip list``` ). \n",
    "\n",
    "What is version of the package *wheel* ?\n",
    "\n",
    "- **0.42.0**\n",
    "- 1.0.0\n",
    "- 23.0.1\n",
    "- 58.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use` --entrypoint=bash` to override the default entrypoint of the container and run the Bash shell, allowing for manual operations like installing Python packages. The entrypoint is the command that Docker runs by default when the container starts.\n",
    "\n",
    "```bash\n",
    "    docker run -it --entrypoint=bash python:3.9\n",
    "```\n",
    "After run the cl, inside of the container, run `pip list` to check the python modules that are installed:\n",
    "\n",
    "```bash\n",
    "    root@1434e4eadd55:/# pip list\n",
    "    Package    Version\n",
    "    ---------- -------\n",
    "    pip        23.0.1\n",
    "    setuptools 58.1.0\n",
    "    wheel      0.42.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Postgres**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset:**\n",
    "\n",
    "We'll use the green taxi trips from September 2019:\n",
    "\n",
    "```wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz```\n",
    "\n",
    "and the dataset zones:\n",
    "\n",
    "```wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Running Postgres in Docker with pgcli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The image we'll use is called `postgres`. We'll use the latest version, which is `13`. We need to configure the following environment variables:\n",
    "\n",
    "- `POSTGRES_USER` - the username for the database\n",
    "- `POSTGRES_PASSWORD` - the password for the database\n",
    "- `POSTGRES_DB` - the name of the database\n",
    "\n",
    "We need to use the flags `-e` and `-v` to set environment variables and mount volumes, respectively. The `-v` format is `-v host_path:container_path`, and PostgreSQL by default is listen on port 5432. Also, with `-p` flag we can expose the port from the container to the host. The format is `-p host_port:container_port`, and the default data directory in docker is `/var/lib/postgresql/data`. The full command is:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER=\"marcosbenicio\" \\\n",
    "    -e POSTGRES_PASSWORD=\"0102\" \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "    -v $(pwd)/taxi-trip-postgres:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432 \\\n",
    "    postgres:13\n",
    "```\n",
    "\n",
    "The necessity to specify a volume is because the data is stored in the container, and when the container is removed, the data is lost. So, we need to mount a volume to persist the data. \n",
    "\n",
    "To easily run this command, we can create a run-postgres.sh file with the docker command and turn into a executable file with:\n",
    "\n",
    "```bash\n",
    "    chmod +x run-postgres.sh\n",
    "```\n",
    "\n",
    "Then, we can run the executable file with:\n",
    "\n",
    "```bash\n",
    "    ./run-postgres.sh\n",
    "```\n",
    "\n",
    "To interact with the database from terminal, we can install `pgcli`:\n",
    "\n",
    "```bash\n",
    "    pip install pgcli\n",
    "```\n",
    "and connect to the database:\n",
    "\n",
    "```bash\n",
    "    pgcli -h localhost -p 5432 -U marcosbenicio -d ny_taxi\n",
    "```\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/pgcli-connection.png\" alt=\"drawing\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Ingesting Data Into Postgres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To ingest data into Postgres, we can use the sql alchemy library. First, we need to install it:\n",
    "\n",
    "```bash\n",
    "    pip install sqlalchemy\n",
    "```\n",
    "Then, we can use the following code to ingest the data:\n",
    "\n",
    "```python\n",
    "    engine  = create_engine('posgresql://username:password@localhost:port/database_name')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE green_taxi_trip (\n",
      "\t\"VendorID\" FLOAT(53), \n",
      "\tlpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tlpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\tehail_fee FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tpayment_type FLOAT(53), \n",
      "\ttrip_type FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create a connection to the database\n",
    "engine = create_engine('postgresql://marcosbenicio:0102@localhost:5432/ny_taxi')\n",
    "\n",
    "\n",
    "df_taxi_trip = pd.read_csv( 'data/green-taxi-trip-2019-09.csv', \n",
    "                            parse_dates=['lpep_pickup_datetime', 'lpep_dropoff_datetime'], \n",
    "                            low_memory=False)\n",
    "\n",
    "\n",
    "# Create a table in database from the pandas dataframe\n",
    "df_taxi_trip.to_sql( name = \"green_taxi_trip\", con = engine, if_exists = \"replace\")\n",
    "\n",
    "# Create a ddl schema from the pandas dataframe\n",
    "print(pd.io.sql.get_schema(df_taxi_trip, name = \"green_taxi_trip\", con = engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the existing tables in the database from the terminal using the `\\dt` command or a query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"figures/cl-posgresql-tables.png\" alt=\"drawing\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PostgreSQL, the information about the tables, including their structure, columns, and other metadata, is stored in a kind of system catalogs within the `information_schema` and `pg_catalog` schemas. These are special schemas that hold metadata about the database.\n",
    "\n",
    "To remove a table from our database, we can use the `DROP TABLE` command:\n",
    "\n",
    "```sql\n",
    "    DROP TABLE green_taxi_trip;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Running Postgres and pgAdmin with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not convenient to use the pgcli for data exploration and querying. Instead, we can use pgAdmin, which is a web-based interface for managing PostgreSQL databases. We can download the image of pgAdmin from docker hub:\n",
    "\n",
    "```bash\n",
    "    docker pull dpage/pgadmin4\n",
    "```\n",
    "\n",
    "Then, we can run the docker container with the following environment variables, similar to the postgres container:\n",
    "\n",
    "```bash\n",
    "    docker run -it \\\n",
    "        -e PGADMIN_DEFAULT_EMAIL=\"marcosbenicio@admin.com\" \\\n",
    "        -e PGADMIN_DEFAULT_PASSWORD=\"0102\" \\\n",
    "        -p 8080:80 \\\n",
    "        dpage/pgadmin4\n",
    "```\n",
    "\n",
    "Again, to easily run this command, we can create a run-pgadmin.sh file with the docker command and turn into a executable file with:\n",
    "\n",
    "```bash\n",
    "    chmod +x run-pgadmin.sh\n",
    "```\n",
    "\n",
    "Then, we can run the executable file with:\n",
    "\n",
    "```bash\n",
    "    ./run-pgadmin.sh\n",
    "```\n",
    "This will download the image if it's not already downloaded and run the container with the defined environment variables.\n",
    "\n",
    "Now we need to create a connection  between the postgres container and the Pgadmin container. To do that, we need to create a network between the containers. We can do that with the following docker command:\n",
    "\n",
    "```bash\n",
    "    docker network create pg-network\n",
    "```\n",
    "where `pg-network` is the name give to our network that will connect the postgres to Pgadmin. We can list all the networks with the command:\n",
    "\n",
    "```bash\n",
    "    docker network ls\n",
    "```\n",
    "\n",
    "The result is the following \n",
    "\n",
    "<center>\n",
    "<img src=\"figures/network-create-list.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "There are three networks by default and one that we created. The default networks are:\n",
    "\n",
    "- Bridge Network: This is the default network that Docker containers connect to if no other network is specified. \n",
    "\n",
    "- Host Network: When you use this network, your container shares the network stack of the host.\n",
    "\n",
    "- None Network: This is a null network, used when we don't want the container to have networking capabilities. Containers attached to this network are completely isolated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove all containers created before with `docker rm -f <container-id>` and run again the postgres and the pgadmin container in this new network with the following commands:\n",
    "\n",
    "- For postgres, is exactly the same command as before with the addition of the `--network <network-name>` flag and `--name` flag to give a name to the container. This name is how PgAdmin will recognize the postgres container. Also, we change the flag `-it` to `-d` to run the container in the background.\n",
    "\n",
    "```bash\n",
    "    docker run -d \\\n",
    "        -e POSTGRES_USER=\"marcosbenicio\" \\\n",
    "        -e POSTGRES_PASSWORD=\"0102\" \\\n",
    "        -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "        -v $(pwd)/taxi-trip-postgres:/var/lib/postgresql/data \\\n",
    "        -p 5432:5432 \\\n",
    "        --network=pg_network\n",
    "        --name pg-database\n",
    "        postgres:13\n",
    "```\n",
    "\n",
    "- For pgadmin, we specify again the same network `pg-network`. The name flag is less important here, because the connection is made from the postgres container to the pgadmin container. \n",
    "\n",
    "```bash\n",
    "    docker run -d \\\n",
    "        -e PGADMIN_DEFAULT_EMAIL=\"marcosbenicio@admin.com\" \\\n",
    "        -e PGADMIN_DEFAULT_PASSWORD=\"0102\" \\\n",
    "        -p 8080:80 \\\n",
    "        --network=pg-network \\\n",
    "        --name pgadmin-interface \\\n",
    "        dpage/pgadmin4\n",
    "```\n",
    "Note that if the PostgreSQL container was started previously without the `POSTGRES_DB` environment variable, or if the database was deleted, it will not recreate the database or the table. In such cases, the database and the table must be created manually with the following SQL command:\n",
    "\n",
    "```sql\n",
    "    CREATE DATABASE ny_taxi;\n",
    "```\n",
    "\n",
    "Again, to easily run this command, we can create a `postgres-pgadmin-connection.sh` file with both docker commands and turn into a executable file with:\n",
    "\n",
    "```bash\n",
    "    chmod +x postgres-pgadmin-connection.sh\n",
    "    ./postgres-pgadmin-connection.sh\n",
    "```\n",
    "\n",
    "Let's inspect our network with the command:\n",
    "\n",
    "```bash\n",
    "    docker network inspect pg-network\n",
    "```\n",
    "\n",
    "The result is the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/network-inspect.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "We can see that both the containers are connected to the network `pg-network`, as shown inside the containers variable.\n",
    "\n",
    "Now we can access our localhost on port 8080 with `http://localhost:8080/` via browser:\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/pdamin-entry.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "After login, we can register a new server with host `pg-database` (the name of the postgres container) as show bellow. The username and password are the same as defined in the environment variables. The port is 5432, the default port of postgres.  \n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/register-server.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "Then, we can save and access the database.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Dockerizing the data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to ingest the data into postgres is creating a python script. This way the process of download and ingest data into postgres can be automated. The script file is the following:\n",
    "\n",
    "**ingest-data.py**\n",
    "```python\n",
    "    import os\n",
    "    import argparse\n",
    "    from sqlalchemy import create_engine\n",
    "    import pandas as pd\n",
    "    import gzip\n",
    "\n",
    "    def main(params):\n",
    "        \n",
    "        try:\n",
    "            # Get the parameters \n",
    "            user = params.user\n",
    "            password = params.password\n",
    "            host = params.host\n",
    "            port = params.port\n",
    "            database_name = params.database_name\n",
    "            table_name = params.table_name\n",
    "            url = params.url\n",
    "            file_name = params.file_name\n",
    "            file_extension = params.file_extension\n",
    "            \n",
    "            # Download csv file from url\n",
    "            if os.system(f'wget {url} -O {file_name}') != 0:\n",
    "                raise Exception(f\"Failed to download file from {url}\")\n",
    "\n",
    "            # Check the file extension and process\n",
    "            if file_extension == '.csv':\n",
    "                df = pd.read_csv(file_name, low_memory=False) \n",
    "            \n",
    "            elif file_extension == '.gz':\n",
    "                with gzip.open(file_name, 'rb') as f:\n",
    "                    df = pd.read_csv(f, low_memory=False)\n",
    "        \n",
    "            else:\n",
    "                raise Exception(f\"File extension {file_extension} not supported\")\n",
    "            \n",
    "            # Create a connection to the database\n",
    "            engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{database_name}')\n",
    "            \n",
    "            # Create a table in database from the pandas dataframe\n",
    "            df.to_sql(name=table_name, con=engine, if_exists=\"replace\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":  \n",
    "\n",
    "        parser = argparse.ArgumentParser(description='Ingest CSV data to postgres')\n",
    "\n",
    "        parser.add_argument(\"--user\", help= 'user name for postgres' )\n",
    "        parser.add_argument(\"--password\", help= 'password for postgres' )\n",
    "        parser.add_argument(\"--host\", help= 'host for postgres')\n",
    "        parser.add_argument(\"--port\", help= 'port for postgres')\n",
    "        parser.add_argument(\"--database_name\", help= 'database name for postgres')\n",
    "        parser.add_argument(\"--url\", help= 'url for file csv or .gz file to ingest')\n",
    "        parser.add_argument(\"--table_name\", help= 'Table name to pass data')\n",
    "        parser.add_argument(\"--file_name\", help= 'Name to save data')\n",
    "        parser.add_argument(\"--file_extension\", help= 'Extension of the file to ingest (csv or .gz)')\n",
    "\n",
    "        args = parser.parse_args()\n",
    "        main(args)\n",
    "```\n",
    "------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to automatically download and ingest the data with the script create the docker file:\n",
    "\n",
    "**Dockerfile**\n",
    "```dockerfile\n",
    "        FROM python:3.9.1\n",
    "\n",
    "        RUN apt-get update && apt-get install -y wget\n",
    "        RUN pip install pandas sqlalchemy psycopg2 pyarrow\n",
    "\n",
    "        WORKDIR /app\n",
    "\n",
    "        # Copy the script directly into /app\n",
    "        COPY [\"ingest-data.py\", \"./\"]\n",
    "\n",
    "        # Execute ingest-data.py when the container starts\n",
    "        ENTRYPOINT [ \"python\", \"ingest-data.py\" ]\n",
    "```\n",
    "\n",
    "\n",
    "Then, build the docker image containing the python script with the following command:\n",
    "\n",
    "```bash\n",
    "    docker build -t ingest-data:v01 .\n",
    "```\n",
    "Now that we build the docker image, we can run the container with the same network as the Postgres and Pgadmin containers. To do so, add the `--network <network-name>` flag to the docker run command with the defined variables from our script. The command can be written inside a executable file as:\n",
    "\n",
    "**ingest-data-postgres.sh**\n",
    "\n",
    "```bash\n",
    "        URL1=\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz\"\n",
    "        URL2=\"https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\"\n",
    "\n",
    "        # Define parameters\n",
    "        USER=\"marcosbenicio\"\n",
    "        PASSWORD=\"0102\"\n",
    "        HOST=\"pg-database\"\n",
    "        PORT=\"5432\"\n",
    "        DATABASE_NAME=\"ny_taxi\"\n",
    "        TABLE_NAME1=\"green_taxi_trip\"\n",
    "        TABLE_NAME2=\"taxi_zone_lookup\"\n",
    "        FILE_NAME1=\"green-tripdata-2019-09\"\n",
    "        FILE_NAME2=\"taxi-zone-lookup\"\n",
    "        FILE_EXTENSION1=\".gz\"\n",
    "        FILE_EXTENSION2=\".csv\"\n",
    "\n",
    "        # Ingest first dataset\n",
    "        docker run -it --rm\\\n",
    "            --network pg-network \\\n",
    "            ingest-data:v01 \\\n",
    "                --user=${USER} \\\n",
    "                --password=${PASSWORD} \\\n",
    "                --host=${HOST} \\\n",
    "                --port=${PORT} \\\n",
    "                --database_name=${DATABASE_NAME} \\\n",
    "                --table_name=${TABLE_NAME1} \\\n",
    "                --url=${URL1} \\\n",
    "                --file_name=${FILE_NAME1} \\\n",
    "                --file_extension=${FILE_EXTENSION1}\n",
    "\n",
    "        # Ingest second dataset\n",
    "        docker run -it --rm\\\n",
    "            --network pg-network \\\n",
    "            ingest-data:v01 \\\n",
    "                --user=${USER} \\\n",
    "                --password=${PASSWORD} \\\n",
    "                --host=${HOST} \\\n",
    "                --port=${PORT} \\\n",
    "                --database_name=${DATABASE_NAME} \\\n",
    "                --table_name=${TABLE_NAME2} \\\n",
    "                --url=${URL2} \\\n",
    "                --file_name=${FILE_NAME2} \\\n",
    "                --file_extension=${FILE_EXTENSION2}\n",
    "                \n",
    "```\n",
    "-----\n",
    "\n",
    "The `docker run` command create a container every time we run it, so is needed to add the `--rm` flag to remove the container after the execution. This is useful to avoid the creation of multiple containers.\n",
    "\n",
    "If the table was dropped before, first recreate the database and table as in the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/pgadmin-create-table.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "Finally, run the executable file with:\n",
    "\n",
    "```bash\n",
    "    chmod +x ingest-data-postgres.sh\n",
    "    ./ingest-data-postgres.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Running Postgres and pgAdmin with Docker-Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing all the steps above, we can use docker-compose to run the container with a single file. With Compose, we use a YAML file to configure our application's services without the need of the `postgres-pgadmin-connection.sh` or the `ingest-data.sh`. The docker-compose files are the following:\n",
    "\n",
    "\n",
    "**docker-compose.yaml**\n",
    "```yaml\n",
    "    services:\n",
    "      pg-database:\n",
    "        image: postgres:13\n",
    "        environment:\n",
    "          - POSTGRES_USER=marcosbenicio\n",
    "          - POSTGRES_PASSWORD=0102\n",
    "          - POSTGRES_DB=ny_taxi\n",
    "        volumes: # mount a volume to persist the data\n",
    "          - \"./taxi-trip-postgres:/var/lib/postgresql/data:rw\"\n",
    "        ports: # default port for postgres\n",
    "          - \"5432:5432\"\n",
    "\n",
    "      pgadmin:\n",
    "        image: dpage/pgadmin4\n",
    "        environment:\n",
    "          - PGADMIN_DEFAULT_EMAIL=marcosbenicio@admin.com\n",
    "          - PGADMIN_DEFAULT_PASSWORD=0102\n",
    "        ports:\n",
    "          - \"8080:80\"\n",
    "        networks: # connect to the same network as postgres\n",
    "          - default\n",
    "          \n",
    "    networks: # create a network to connect the containers\n",
    "      default:\n",
    "        external:\n",
    "          name: pg-network\n",
    "```\n",
    "-----\n",
    "\n",
    "**docker-compose.ingest.yaml**\n",
    "```yaml\n",
    "  services:\n",
    "    data-ingest-1:\n",
    "      image: ingest-data:v01\n",
    "      command: > # it's not necessary to use an \"=\" or \":\" sign here to pass the variables\n",
    "        --user marcosbenicio\n",
    "        --password 0102\n",
    "        --host pg-database\n",
    "        --port 5432\n",
    "        --database_name ny_taxi\n",
    "        --table_name green_taxi_trip\n",
    "        --url https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-09.csv.gz\n",
    "        --file_name green-tripdata-2019-09\n",
    "        --file_extension .gz\n",
    "      networks:\n",
    "        - default\n",
    "\n",
    "    data-ingest-2:\n",
    "      image: ingest-data:v01\n",
    "      command: >\n",
    "        --user marcosbenicio\n",
    "        --password 0102\n",
    "        --host pg-database\n",
    "        --port 5432\n",
    "        --database_name ny_taxi\n",
    "        --table_name taxi_zone_lookup\n",
    "        --url https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
    "        --file_name taxi-zone-lookup\n",
    "        --file_extension .csv\n",
    "      networks:\n",
    "        - default\n",
    "\n",
    "  networks:\n",
    "    default:\n",
    "      external:\n",
    "        name: pg-network\n",
    "```\n",
    "-----\n",
    "\n",
    "If we do not specify the network, docker-compose will create a new network for us for each docker-compose file. To run the docker-compose files, both to start the database service and ingest data, we need to connect to the same network as the postgres container. To do so, we need to create the network before running the docker-compose files if the network does not exist. To so do run the following command: \n",
    "\n",
    "```bash\n",
    "    docker network create pg-network\n",
    "```\n",
    "\n",
    "Then, we can first run the `docker-compose.yaml` file to start our database services:\n",
    "\n",
    "```bash\n",
    "    docker-compose up -d\n",
    "```\n",
    "\n",
    "and after started the containers, we can run the `docker-compose.ingest.yaml` file to ingest the data:\n",
    "\n",
    "```bash\n",
    "    docker-compose -f docker-compose.ingest.yaml up\n",
    "```\n",
    "To stop the containers, we can type the following command:\n",
    "\n",
    "```bash\n",
    "    docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Homework: using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Question 3. Count records**\n",
    "\n",
    "How many taxi trips were totally made on September 18th 2019?\n",
    "\n",
    "Tip: started and finished on 2019-09-18. \n",
    "\n",
    "Remember that `lpep_pickup_datetime` and `lpep_dropoff_datetime` columns are in the format timestamp (date and hour+min+sec) and not in date.\n",
    "\n",
    "- 15767\n",
    "- **15612**\n",
    "- 15859\n",
    "- 89009\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-3.png\" alt=\"drawing\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question 4. Largest trip for each day**\n",
    "\n",
    "Which was the pick up day with the largest trip distance\n",
    "Use the pick up time for your calculations.\n",
    "\n",
    "- 2019-09-18\n",
    "- 2019-09-16\n",
    "- **2019-09-26**\n",
    "- 2019-09-21\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-4.png\" alt=\"drawing\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question 5. Three biggest pick up Boroughs**\n",
    "\n",
    "Consider lpep_pickup_datetime in '2019-09-18' and ignoring Borough has Unknown\n",
    "\n",
    "Which were the 3 pick up Boroughs that had a sum of total_amount superior to 50000?\n",
    " \n",
    "- **\"Brooklyn\" \"Manhattan\" \"Queens\"**\n",
    "- \"Bronx\" \"Brooklyn\" \"Manhattan\"\n",
    "- \"Bronx\" \"Manhattan\" \"Queens\" \n",
    "- \"Brooklyn\" \"Queens\" \"Staten Island\"\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-5.png\" alt=\"drawing\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question 6. Largest tip**\n",
    "\n",
    "For the passengers picked up in September 2019 in the zone name Astoria which was the drop off zone that had the largest tip?\n",
    "We want the name of the zone, not the id.\n",
    "\n",
    "Note: it's not a typo, it's `tip` , not `trip`\n",
    "\n",
    "- Central Park\n",
    "- Jamaica\n",
    "- **JFK Airport**\n",
    "- Long Island City/Queens Plaza\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-6.png\" alt=\"drawing\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Terraform**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terraform is an infrastructure as code (IaC) tool used for building, changing, and versioning infrastructure. It manages the infrastructure itself (servers, databases, networks, etc.) in cloud providers (like AWS, Azure, GCP) and on-premises. They use a special program language called HashiCorp Configuration Language (HCL) to define the infrastructure.  It is used to manage the lifecycle of infrastructure resources, including creation, modification, and deletion. Terraform operates at the infrastructure level, managing the platform on which applications run.\n",
    "\n",
    "Terraform is a infrastructure management, while Docker is for deployment and management. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Creating  and configuring a project on Google Cloud Platform (GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use terraform, we first need to create a project in GCP. To do so, we need to go to the [GCP console](https://console.cloud.google.com/) and create a new project. After created the project, we can create our service. Go to `1AM & Admin > Service Accounts` and create a new service account by clicking in `+ CREATE SERVICE ACCOUNT`.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/gcp-service.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The following window will appear to create the service account. Give a name to the service account and click on create:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/gcp-create-service-acc.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "We can grant the service account access to the project by assign roles on the optional step show in the image. If we forgot to assign roles, we can do it later by going to the left menu on `1AM` and clicking on the edit principle button (the pencil icon):\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/gcp-assign-roles.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "To access from our local machine to the GCP project, we need to create a key for the service account. To do so, go to `Service Accounts > Actions` and select `Manage Keys` to generate ssh keys. Then, click on `ADD KEY > Create new key` and select the JSON format.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/gcp-create-keys.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "This will download a JSON file with the credentials for the service account. We can rename the file to `terraform-runner-credentials.json` and move it to the same directory as the terraform files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Configure and Deploy with Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure the [GCP provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs) in Terraform, add the following code to the `main.tf` file:\n",
    "\n",
    "```terraform\n",
    "    terraform {\n",
    "        required_providers {\n",
    "            google = {\n",
    "                source = \"hashicorp/google\"\n",
    "                version = \"5.14.0\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    provider \"google\" {\n",
    "        credentials = \"./keys/terraform-runner-credentials.json\"      # Path to the JSON file credentials.\n",
    "        project = \"terraform-runner-412811\"         # ID of the Google Cloud project.\n",
    "        region  = \"southamerica-east1\"\n",
    "    }\n",
    "```\n",
    "If necessary, we can automatically format the code with the command:\n",
    "\n",
    "```bash\n",
    "  terraform fmt\n",
    "```\n",
    "\n",
    "Now, we need to take the project ID from the GCP console and replace `my-project-id`. for this project the ID is  `terraform-runner-412811`. Also, we can change the region to somewhere more close to our country if necessary. Because I'm from Brazil is reasonable to change to `southamerica-east1` instead of  the default `us-central1`. The `terraform` block defines the required providers for your Terraform project, while `provider \"google\"` block configures the Google Cloud provider with specific settings like credentials, project ID, and the default region for resources.\n",
    "\n",
    "Now, we continue working on the `main.tf` file by adding the `google_storage_bucket` resource block. To do so we can check a example for [GCP clound storage bucket](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/storage_bucket). Is only needed to change the name of the bucket and the location. The final code is the following:\n",
    "\n",
    "```terraform\n",
    "    resource \"google_storage_bucket\" \"demo-bucket\" {\n",
    "        name     = \"terraform-runner-412811-bucket\"     # needs to be unique name across all GCP\n",
    "        location = \"southamerica-east1\"\n",
    "        force_destroy = true\n",
    "        \n",
    "        lifecycle_rule {\n",
    "            condition {\n",
    "                age = 1    # In days\n",
    "            }\n",
    "            action {\n",
    "                type = \"AbortIncompleteMultipartUpload\"\n",
    "            }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The `lifecycle_rule` block is used to define a lifecycle rule for the bucket. In this case, we define a rule to abort incomplete multipart uploads after 1 day. This is useful to avoid unnecessary charges for incomplete uploads.\n",
    "\n",
    "One last block that we need is the `google_bigquery_dataset` resource block. To do so we can check a example for [GCP BigQuery dataset](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset). Is only needed to change the name of the dataset and the location. The final code is the following:\n",
    "\n",
    "```terraform\n",
    "    resource \"google_bigquery_dataset\" \"demo_dataset\" {\n",
    "        dataset_id = \"demo_dataset\"                     \n",
    "        location = \"southamerica-east1\"\n",
    "    }\n",
    "```\n",
    "\n",
    "The `dataset_id` is the name of the dataset. The `location` is the location where the dataset will be created. The default location is `US`. The location can't be changed after the dataset is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a simple terraform file just to create a bucket to deploy a dataset into GCP BigQuery. Before running the code, we can create another file named `variable.tf` to define variables used in the `main.tf` file. This is useful to avoid hardcoding values in the code. \n",
    "\n",
    "**variable.tf**\n",
    "\n",
    "```terraform\n",
    "      variable \"project\" {\n",
    "          description = \"default project name that Terraform will use\"\n",
    "          default = \"terraform-runner-412811\"  \n",
    "      }\n",
    "\n",
    "      variable \"region\" {\n",
    "          description = \"Sets the default region for the infrastructure deployment\"\n",
    "          default = \"southamerica-east1\" \n",
    "      }\n",
    "\n",
    "      variable \"location\" {\n",
    "          description = \"Indicates the location for the resources. Often similar to or the same as the region.\"\n",
    "          default = \"southamerica-east1\" \n",
    "      }\n",
    "\n",
    "      variable \"bq_dataset_name\" {\n",
    "          description = \"Names the BigQuery Dataset\"\n",
    "          default = \"demo_dataset\" \n",
    "      }\n",
    "\n",
    "      variable \"gcs_bucket_name\" {\n",
    "          description = \"Name of a Google Cloud Storage (GCS) bucket.\"\n",
    "          default = \"terraform-runner-412811-bucket\" \n",
    "      }\n",
    "\n",
    "      variable \"gcs_storage_class\" {\n",
    "          description = \"Determines the storage class of the GCS bucket\"\n",
    "          default = \"STANDARD\"        # common values: STANDARD, NEARLINE, COLDLINE\n",
    "      }\n",
    "```\n",
    "---------\n",
    "\n",
    "Each variable has a name, a type, and a default value. \n",
    "\n",
    "**main.tf**\n",
    "\n",
    "```terraform\n",
    "        terraform {\n",
    "          required_providers {\n",
    "            google = {\n",
    "              source  = \"hashicorp/google\"\n",
    "              version = \"5.14.0\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "\n",
    "        provider \"google\" {\n",
    "          credentials = file(var.credentials)       # load content of JSON file credentials.\n",
    "          project     = var.project                 # ID of the Google Cloud project.\n",
    "          region      = var.region                  # Region where resources will be deployed.\n",
    "        }\n",
    "\n",
    "        resource \"google_storage_bucket\" \"demo-bucket\" {\n",
    "          name          = var.gcs_bucket_name        # needs to be unique name across all GCP\n",
    "          location      = var.location\n",
    "          force_destroy = true\n",
    "\n",
    "          lifecycle_rule {\n",
    "            condition {\n",
    "              age = 1             # In days\n",
    "            }\n",
    "            action {\n",
    "              type = \"AbortIncompleteMultipartUpload\"\n",
    "            }\n",
    "          } \n",
    "        }\n",
    "\n",
    "        resource \"google_bigquery_dataset\" \"demo_dataset\" {\n",
    "          dataset_id = var.bq_dataset_name\n",
    "          location   = var.location\n",
    "        }\n",
    "```\n",
    "-------\n",
    "\n",
    "\n",
    "Now, we can run the terraform commands to initialize the project, plan the deployment, and apply the changes. To initialize the project, run the following command:\n",
    "\n",
    "```bash\n",
    "    terraform init\n",
    "```\n",
    "\n",
    "this will a lock file `.terraform.lock.hcl` and a hidden directory `.terraform`. The lock file is used to track the versions of the providers used in the project. The hidden directory contains the plugins and modules used in the project.\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/terraform-init.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "To display what will be create we can run the following command:\n",
    "\n",
    "```bash\n",
    "    terraform plan\n",
    "```\n",
    "\n",
    "We can see that the plan will create a bucket and a dataset in GCP:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/terraform-plan.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "And finally, we can deploy the resources with the following command to create what was planned. If an Error 403 is returned, is need to enable the BigQuery API in GCP console by going into `APIs & Services > Enabled APIs & Services` and enabling `BigQuery API` .\n",
    "\n",
    "```bash\n",
    "    terraform apply\n",
    "```\n",
    "\n",
    "After the deployment, we can check the resources created in GCP console and upload our dataset to the bucket. \n",
    "\n",
    "<center>\n",
    "<img src=\"figures/bucket-clound.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To destroy the resources created, we can run the following command:\n",
    "\n",
    "```bash\n",
    "    terraform destroy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Homework: Terraform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Question 7. Creating Resources**\n",
    "\n",
    "After updating the main.tf and variable.tf files run:\n",
    "\n",
    "```\n",
    "terraform apply\n",
    "```\n",
    "\n",
    "Paste the output of this command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "  Plan: 2 to add, 0 to change, 0 to destroy.\n",
    "\n",
    "  Do you want to perform these actions?\n",
    "    Terraform will perform the actions described above.\n",
    "    Only 'yes' will be accepted to approve.\n",
    "\n",
    "    Enter a value: yes\n",
    "\n",
    "  google_bigquery_dataset.demo_dataset: Creating...\n",
    "  google_storage_bucket.demo-bucket: Creating...\n",
    "  google_storage_bucket.demo-bucket: Creation complete after 2s [id=terraform-runner-412811-bucket]\n",
    "  google_bigquery_dataset.demo_dataset: Creation complete after 2s [id=projects/terraform-runner-412811/datasets/demo_dataset]\n",
    "\n",
    "  Apply complete! Resources: 2 added, 0 changed, 0 destroyed.\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
