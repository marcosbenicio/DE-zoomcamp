{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outline**\n",
    "\n",
    "- [**1. Data Warehouse**](#1.-Data-Warehouse)\n",
    "- [**Homework**](#-Homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Warehouse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before entering into the concept of Data Warehouse, first lets understand what is OLAP (Online Analytical Processing) and OLTP (Online Transaction Processing). They are two distinct types of data processing systems used in the context of data warehousing and database management, each designed to support different kinds of workloads and user requirements. Summarizing the differences between OLAP and OLTP:\n",
    "\n",
    "**OLTP**\n",
    "\n",
    "OLTP systems are designed to manage transaction-oriented applications. They are optimized for handling a large number of short, atomic (indivisible) transactions. These systems are commonly used in day-to-day operations of businesses, such as sales transactions, banking, etc.\n",
    "\n",
    "**OLAP**\n",
    "\n",
    "OLAP systems are designed for complex queries and analyses, rather than transaction processing. They support decision-making and strategic planning by facilitating the manipulation and analysis of large volumes of data from different perspectives.\n",
    "\n",
    "\n",
    "|                    | OLTP                                            | OLAP                                              |\n",
    "|--------------------|-------------------------------------------------|---------------------------------------------------|\n",
    "| **Purpose**        | Control and run essential business operations in real time | Plan, solve problems, support decisions, discover hidden insights |\n",
    "| **Data updates**   | Short, fast updates initiated by user           | Data periodically refreshed with scheduled, long-running batch jobs |\n",
    "| **Database design**| Normalized databases for efficiency             | Denormalized databases for analysis               |\n",
    "| **Space requirements** | Generally small if historical data is archived | Generally large due to aggregating large datasets |\n",
    "| **Backup and recovery** | Regular backups required to ensure business continuity and meet legal and governance requirements | Lost data can be reloaded from OLTP database as needed in lieu of regular backups |\n",
    "| **Productivity**   | Increases productivity of end users                        | Increases productivity of business managers, data analysts, and executives |\n",
    "| **Data view**      | Lists day-to-day business transactions                     | Multi-dimensional view of enterprise data                  |\n",
    "| **User examples**  | Customer-facing personnel, clerks, online shoppers         | Knowledge workers such as data analysts, business analysts, and executives |\n",
    "\n",
    "The data wherehouse is a system used for reporting and data analysis, and is considered a core component of business intelligence. Data wherehouses are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating trending reports for senior management reporting such as annual and quarterly comparisons.\n",
    "\n",
    "\n",
    "Data warehouse is associated with OLAP. It is designed to support the OLAP processes, which are essential for performing complex queries and analyses. The data warehouse provides a centralized repository of integrated data from multiple sources. Data within a data warehouse is structured specifically for query and analysis purposes, often using denormalized to optimize for read operations and analytical queries rather than transactional speed.\n",
    "\n",
    "Consider the following diagram to understand the concept of Data Warehouse:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/wherehouse-diagram.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "- **Data Source** : Data sources are systems that provide data to the data warehouse. These can be operational systems (or transactional system), relational databases, flat files (text files, CSVs..), or other sources of data. \n",
    "\n",
    "- **Staging Area** : Is a temporary storage space where data from different sources is collected before being processed and loaded into the data warehouse.\n",
    "\n",
    "- **Data Warehouse** : The central component of the diagram is the data warehouse itself, which is typically a relational database designed for query and analysis rather than transaction processing. inside the warehouse, the diagram shows three types of data:\n",
    "\n",
    "    - Meta Data: This is data about data. It contains information about the structure, formatting, and relationships within the warehouse.\n",
    "\n",
    "    - Summary Data: Aggregated or calculated data, often precomputed to speed up common queries.\n",
    "\n",
    "    - Raw Data: The detailed data transferred from the staging area into the warehouse without any aggregation or summarization.\n",
    "\n",
    "- **Data Marts**: are subsets of the data warehouse and are usually oriented to a specific business line or team. They contain a subset of the warehouse data relevant to a particular group or business function.  The diagram lists three examples: purchasing, sales, and inventory.\n",
    "\n",
    "- **Users**: Finally, the diagram shows the end users of the data warehouse and data marts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Important Note:</b></u> <p> For this homework we will be using the 2022 Green Taxi Trip Record Parquet Files from the New York\n",
    "City Taxi Data found here: </br> https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page </br>\n",
    "If you are using orchestration such as Mage, Airflow or Prefect do not load the data into Big Query using the orchestrator.</br> \n",
    "Stop with loading the files into a bucket. </br></br>\n",
    "<u>NOTE:</u> You will need to use the PARQUET option files when creating an External Table</br>\n",
    "\n",
    "<b>SETUP:</b></br>\n",
    "Create an external table using the Green Taxi Trip Records Data for 2022. </br>\n",
    "Create a table in BQ using the Green Taxi Trip Records for 2022 (do not partition or cluster this table). </br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1:\n",
    "Question 1: What is count of records for the 2022 Green Taxi Data??\n",
    "- 65,623,481\n",
    "- **840,402**\n",
    "- 1,936,423\n",
    "- 253,647\n",
    "\n",
    "To get the count of records for the 2022 Green Taxi Data, we can use the following query:\n",
    "\n",
    "\n",
    "```sql\n",
    "    SELECT COUNT(*) AS total_records\n",
    "    FROM `de-bootcamp-414215.taxi_data.green_taxi_external_2022`;\n",
    "```\n",
    "the result is the following:\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-1.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "## Question 2:\n",
    "Write a query to count the distinct number of PULocationIDs for the entire dataset on both the tables.</br> \n",
    "What is the estimated amount of data that will be read when this query is executed on the External Table and the Table?\n",
    "\n",
    "- **0 MB for the External Table and 6.41MB for the Materialized Table**\n",
    "- 18.82 MB for the External Table and 47.60 MB for the Materialized Table\n",
    "- 0 MB for the External Table and 0MB for the Materialized Table\n",
    "- 2.14 MB for the External Table and 0MB for the Materialized Table\n",
    "\n",
    "\n",
    "Create a materialized table from the external one using:\n",
    "\n",
    "```sql\n",
    "    CREATE OR REPLACE TABLE `de-bootcamp-414215.taxi_data.materialized_green_taxi_2022`\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM `de-bootcamp-414215.taxi_data.green_taxi_external_2022`;\n",
    "```\n",
    "\n",
    "Then, we can use the following query to get the count of distinct PULocationIDs for the entire dataset on both the tables:\n",
    "\n",
    "```sql\n",
    "    SELECT COUNT(DISTINCT PULocationID) AS distinct_PULocationIDs\n",
    "    FROM `de-bootcamp-414215.taxi_data.green_taxi_external_2022`;\n",
    "```\n",
    "\n",
    "then for the materialized table:\n",
    "\n",
    "```sql\n",
    "    SELECT COUNT(DISTINCT PULocationID) AS distinct_PULocationIDs\n",
    "    FROM `de-bootcamp-414215.taxi_data.materialized_green_taxi_2022`;\n",
    "```\n",
    "The result should be the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-2.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "## Question 3:\n",
    "How many records have a fare_amount of 0?\n",
    "- 12,488\n",
    "- 128,219\n",
    "- 112\n",
    "- **1,622**\n",
    "\n",
    "To get the number of records that have a fare_amount of 0, we can use the following query:\n",
    "\n",
    "```sql\n",
    "    SELECT COUNT(*) AS records_with_zero_fare\n",
    "    FROM `de-bootcamp-414215.taxi_data.green_taxi_external_2022`\n",
    "    WHERE fare_amount = 0;\n",
    "```\n",
    "\n",
    "the result should be the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-3.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "## Question 4:\n",
    "What is the best strategy to make an optimized table in Big Query if your query will always order the results by PUlocationID and filter based on lpep_pickup_datetime? (Create a new table with this strategy)\n",
    "- Cluster on lpep_pickup_datetime Partition by PUlocationID\n",
    "- **Partition by lpep_pickup_datetime  Cluster on PUlocationID**\n",
    "- Partition by lpep_pickup_datetime and Partition by PUlocationID\n",
    "- Cluster on by lpep_pickup_datetime and Cluster on PUlocationID\n",
    "\n",
    "To create a partitioned table and then cluster it, we can use the following query:\n",
    "\n",
    "```sql\n",
    "    CREATE TABLE `de-bootcamp-414215.taxi_data.green_taxi_partitioned_2022`\n",
    "    PARTITION BY DATE(lpep_pickup_datetime)\n",
    "    CLUSTER BY PUlocationID\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM `de-bootcamp-414215.taxi_data.green_taxi_external_2022`;\n",
    "```\n",
    "\n",
    "the result should bb the following:\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-4.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "## Question 5:\n",
    "Write a query to retrieve the distinct PULocationID between lpep_pickup_datetime\n",
    "06/01/2022 and 06/30/2022 (inclusive)</br>\n",
    "\n",
    "Use the materialized table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 4 and note the estimated bytes processed. What are these values? </br>\n",
    "\n",
    "Choose the answer which most closely matches.</br> \n",
    "\n",
    "- 22.82 MB for non-partitioned table and 647.87 MB for the partitioned table\n",
    "- **12.82 MB for non-partitioned table and 1.12 MB for the partitioned table**\n",
    "- 5.63 MB for non-partitioned table and 0 MB for the partitioned table\n",
    "- 10.31 MB for non-partitioned table and 10.31 MB for the partitioned table\n",
    "\n",
    "Partitioned the materialized table as in question 4:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE `de-bootcamp-414215.taxi_data.materialized_green_taxi_partitioned_2022`\n",
    "PARTITION BY DATE(lpep_pickup_datetime)\n",
    "CLUSTER BY PUlocationID\n",
    "AS\n",
    "SELECT *\n",
    "FROM `de-bootcamp-414215.taxi_data.materialized_green_taxi_2022`;\n",
    "```\n",
    "\n",
    "Then, we can use the following query to get the count of distinct PULocationIDs between lpep_pickup_datetime 06/01/2022 and 06/30/2022 (inclusive) for the partitioned and non partitioned materialized tables:\n",
    "\n",
    "```sql\n",
    "    SELECT COUNT(DISTINCT PULocationID) AS distinct_PULocationIDs\n",
    "    FROM `de-bootcamp-414215.taxi_data.materialized_green_taxi_2022`\n",
    "    WHERE lpep_pickup_datetime BETWEEN '2022-06-01' AND '2022-06-30';\n",
    "```\n",
    "and\n",
    "\n",
    "```sql\n",
    "    SELECT COUNT(DISTINCT PULocationID) AS distinct_PULocationIDs\n",
    "    FROM `de-bootcamp-414215.taxi_data.materialized_green_taxi_partitioned_2022`\n",
    "    WHERE lpep_pickup_datetime BETWEEN '2022-06-01' AND '2022-06-30';\n",
    "```\n",
    "the result should be the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/question-5.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "## Question 6: \n",
    "Where is the data stored in the External Table you created?\n",
    "\n",
    "- Big Query\n",
    "- **GCP Bucket**\n",
    "- Big Table\n",
    "- Container Registry\n",
    "\n",
    "\n",
    "## Question 7:\n",
    "It is best practice in Big Query to always cluster your data:\n",
    "- True\n",
    "- **False**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
