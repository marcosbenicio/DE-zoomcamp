{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outline**\n",
    "\n",
    "- [**1. Configuring Mage with Docker**](#1.-configuring-mage-with-docker)\n",
    "- [**2. ETL Pipeline with Mage and Postgres**](#2.-etl-pipeline-with-mage-and-postgres)\n",
    "    - [2.1 Data Loader](#2.1-data-loader)\n",
    "    - [2.2 Data Transform](#2.2-data-transform)\n",
    "    - [2.3 Data Exporter](#2.3-data-exporter)\n",
    "- [**3. ETL Pipeline with Mage and Google Cloud**](#3.-etl-pipeline-with-mage-and-google-cloud)\n",
    "    - [3.1 Export data to Google Cloud Storage](#3.1-export-data-to-google-cloud-storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Configuring Mage with Docker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Docker and Docker-compose to run the Mage tool for orchestration. First, lets create our docker file by pulling from [docker hub](https://hub.docker.com/r/mageai/mageai) the latest version of mage image:\n",
    "\n",
    "```bash\n",
    "  docker pull mageai/mageai:latest\n",
    "```\n",
    "and then create the docker file:\n",
    "\n",
    "**Dockerfile**\n",
    "```Dockerfile\n",
    "  FROM mageai/mageai:latest\n",
    "\n",
    "  #  For a dynamic path based on environment variable inside .env \n",
    "  ARG USER_CODE_PATH=/home/src/${PROJECT_NAME} \n",
    "\n",
    "  # copy the requirements file to the container\n",
    "  COPY [\"requirements.txt\", \"${USER_CODE_PATH}\"]\n",
    "\n",
    "  # install the requirements\n",
    "  RUN pip3 install -r ${USER_CODE_PATH}/requirements.txt\n",
    "```\n",
    "\n",
    "We use a specific path within the Docker container for our project, separate from our local project folder to leverage Docker's strengths in providing a consistent, isolated, and secure environment for development, testing, and deployment. We also use the `Pipfile` and `Pipfile.lock` files to install the project dependencies using `pipenv`. For the environment variable lets create a `.env` file with the following content:\n",
    "\n",
    "**.env**\n",
    "```bash\n",
    "    PROJECT_NAME=green_taxi\n",
    "    POSTGRES_DBNAME=ny_taxi\n",
    "    POSTGRES_SCHEMA=mage\n",
    "    POSTGRES_USER=marcosbenicio\n",
    "    POSTGRES_PASSWORD=0102\n",
    "    POSTGRES_HOST=postgres\n",
    "    POSTGRES_PORT=5432\n",
    "```\n",
    "\n",
    "For the `docker-compose.yml` file, we will use the following configuration:\n",
    "\n",
    "**docker-compose.yml**\n",
    "```bash\n",
    "    version: '3'\n",
    "    services:\n",
    "\n",
    "      magic:\n",
    "        env_file: # to load environment variables from a file\n",
    "          - .env\n",
    "        build:\n",
    "          context: .   # the context is the current directory in which the docker-compose.yml file is located\n",
    "          dockerfile: Dockerfile  # the Dockerfile to use for building the image\n",
    "        command: mage start ${PROJECT_NAME}\n",
    "        environment:\n",
    "          USER_CODE_PATH: /home/src/${PROJECT_NAME}\n",
    "          POSTGRES_DBNAME: ${POSTGRES_DBNAME}\n",
    "          POSTGRES_SCHEMA: ${POSTGRES_SCHEMA}\n",
    "          POSTGRES_USER: ${POSTGRES_USER}\n",
    "          POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n",
    "          POSTGRES_HOST: ${POSTGRES_HOST}\n",
    "          POSTGRES_PORT: ${POSTGRES_PORT}\n",
    "        ports:\n",
    "          - 6789:6789\n",
    "        volumes: \n",
    "          - .:/home/src/  # copy the current directory to the /home/src directory in the container\n",
    "        restart: on-failure:5\n",
    "\n",
    "      postgres: # must have same name as the environment variable POSTGRES_HOST\n",
    "        image: postgres:14\n",
    "        restart: on-failure\n",
    "        container_name: ${PROJECT_NAME}-postgres\n",
    "        env_file:\n",
    "          - .env\n",
    "        environment:\n",
    "          POSTGRES_DB: ${POSTGRES_DBNAME}\n",
    "          POSTGRES_USER: ${POSTGRES_USER}\n",
    "          POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n",
    "        ports:\n",
    "          - \"${POSTGRES_PORT}:5432\"\n",
    "        volumes: # to persist data after container restarts\n",
    "          - \"./taxi-trip-postgres:/var/lib/postgresql/data:rw\" # mount the local directory to the container\n",
    "```\n",
    "\n",
    "The `docker-compose.yml` file specifies the instructions to build a custom Docker image for the `magic` service using a Dockerfile and then run a container based on that image with the `mage start` command. This command is intended to start the Mage UI and the Mage API within the `magic` container. Separately, the `docker-compose.yml` file also defines a `postgres` service, which runs a PostgreSQL container.\n",
    "\n",
    "The volumes section is used to mount paths from the host machine into the container, allowing for data to be shared between host and the container or for data persistence across container restarts. For `.:/home/src/` we are mounting the current directory (represented by `.`) into the container inside `home/src/` structure.\n",
    "\n",
    "After creating the `Dockerfile`, `.env`, and `docker-compose.yml` files, we can run the following command to build the custom Docker image and start the containers:\n",
    "\n",
    "```bash\n",
    "    docker-compose build\n",
    "\n",
    "    docker-compose up -d\n",
    "```\n",
    "\n",
    "To access Mage UI navigate to `http://localhost:6789` in a browser. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Extract, Transform, Load (ETL) Pipeline with Mage and Postgres**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mage has three main components, know as `blocks`, to create a pipeline: `Data Loader`, `transform`, and `Data Exporter`. The `Data Loader` component is responsible for reading data from a source, the `transform` component is responsible for transforming the data, and the `Data Exporter` component is responsible for writing the data to a destination.\n",
    "\n",
    "To create our connection with the Postgres database, we can create a profile by editing the `io_profile.yaml` file. The `io_profile.yaml` file is used to define the connection to databases. Inside the file we find a pre defined profile named default as example. Based on the examples for connection, we can create a new profile for our Postgres database call dev:\n",
    "\n",
    "**io_profile.yml**\n",
    "```yaml\n",
    "    dev:\n",
    "        POSTGRES_CONNECT_TIMEOUT: 10\n",
    "        POSTGRES_DBNAME: \"{{ env_var('POSTGRES_DBNAME') }}\"\n",
    "        POSTGRES_SCHEMA: \"{{ env_var('POSTGRES_SCHEMA') }}\"\n",
    "        POSTGRES_USER: \"{{ env_var('POSTGRES_USER') }}\"\n",
    "        POSTGRES_PASSWORD: \"{{ env_var('POSTGRES_PASSWORD') }}\"\n",
    "        POSTGRES_HOST: \"{{ env_var('POSTGRES_HOST') }}\"\n",
    "        POSTGRES_PORT: \"{{ env_var('POSTGRES_PORT') }}\"\n",
    "```\n",
    "\n",
    "the `env_var` function is used to access the environment variables defined in the `.env` file. The changes can be made either in vscode or the Mage GUI in our web browser. Now, let's create the pipeline, in the Mage GUI by clicking on the `+` button and selecting the Standard (batch) option as in the figure below:\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/new-pipeline.png\" alt=\"drawing\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Data Loader**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the pipline name we can click on the `Edit > Pipline settings` and change to `green_taxi_etl`. To create a data `Data Loader` block we can click on the `+` button and select the desired block with the python language and then click on the `API` option to create the block:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/data-loader.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The `Data Loader` block is responsible for reading data from a source. For this block, we pre-define the datatypes for a optimization in memory usage and download from this github repo for the [green taxi data from 2019 - 2021](https://github.com/DataTalksClub/nyc-tlc-data/releases/tag/green/download).  We select just the final quarter of 2020 (months `10`, `11`, `12`) using a for loop and then concatenate the dataframes into a single dataframe. The result of this data Loader block would be the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/data-loader-block.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The full code inside this block is:\n",
    "\n",
    "```python\n",
    "    import io\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    if 'data_loader' not in globals():\n",
    "        from mage_ai.data_preparation.decorators import data_loader\n",
    "    if 'test' not in globals():\n",
    "        from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "    @data_loader\n",
    "    def load_data_from_api(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        Template for loading data from API\n",
    "        \"\"\"\n",
    "        dates = ['2020-10', '2020-11', '2020-12']\n",
    "        # Base URL pattern\n",
    "        base_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_{}.csv.gz'\n",
    "        \n",
    "        taxi_dtypes = {\n",
    "            'VendorID': pd.Int64Dtype(),\n",
    "            'passenger_count': pd.Int64Dtype(),\n",
    "            'trip_distance': float,\n",
    "            'RatecodeID': pd.Int64Dtype(),\n",
    "            'store_and_fwd_flag': str,\n",
    "            'PULocationID': pd.Int64Dtype(),\n",
    "            'DOLocationID': pd.Int64Dtype(),\n",
    "            'payment_type': pd.Int64Dtype(),\n",
    "            'fare_amount': float,\n",
    "            'extra': float,\n",
    "            'mta_tax': float,\n",
    "            'tip_amount': float,\n",
    "            'tolls_amount': float,\n",
    "            'improvement_surcharge': float,\n",
    "            'total_amount': float,\n",
    "            'congestion_surcharge': float \n",
    "        }\n",
    "\n",
    "        df_list = []\n",
    "        for date in dates:\n",
    "            # Construct the download URL for each date\n",
    "            url = base_url.format(date)\n",
    "            df = pd.read_csv(url, sep=',', compression='gzip', dtype=taxi_dtypes)\n",
    "            print(f\"Downloading {url}...\")\n",
    "            df_list.append(df)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    @test\n",
    "    def test_output(output, *args) -> None:\n",
    "        \"\"\"\n",
    "        Template code for testing the output of the block.\n",
    "        \"\"\"\n",
    "        assert output is not None, 'The output is undefined'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Data Transform**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created our Data Loader block, we can create the `Data Transform` block. As we did before, we can click on the `+` button and select the desired block with the python language and then click on the `Generic (no template)` option to create the block:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/data-transform.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "The `Data Transform` block is responsible for transforming the data. For this block, we will remove rows where the passenger count is 0 or the trip distance is 0, create a new column 'lpep_pickup_date' by converting 'lpep_pickup_datetime' to a date, and rename columns in Camel Case to Snake Case. The result of this data transform block would be the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/data-transform-block.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The full code inside this block is:\n",
    "\n",
    "\n",
    "```python\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    if 'transformer' not in globals():\n",
    "        from mage_ai.data_preparation.decorators import transformer\n",
    "    if 'test' not in globals():\n",
    "        from mage_ai.data_preparation.decorators import test\n",
    "    \n",
    "    \n",
    "    @transformer\n",
    "    def transform(data, *args, **kwargs):\n",
    "        \n",
    "        # Check is all columns are snake case\n",
    "        def is_snake_case(name):\n",
    "            return re.match(r'^[a-z_][a-z0-9_]*$', name) is not None\n",
    "    \n",
    "        # convert all camel columns names to snake\n",
    "        def camel_to_snake(name):\n",
    "            return re.sub(r'(?<=[a-z0-9])([A-Z])|(?<=[A-Z])([A-Z])(?=[a-z])', r'_\\g<0>', name).lower()\n",
    "        \n",
    "        # Remove rows where the passenger count is 0 or the trip distance is 0.\n",
    "        condition = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)\n",
    "        data = data[condition]\n",
    "        \n",
    "        # Create a new column 'lpep_pickup_date' by converting 'lpep_pickup_datetime' to a date.\n",
    "        data['lpep_pickup_date'] = pd.to_datetime(data['lpep_pickup_datetime']).dt.date.copy()\n",
    "        \n",
    "        # Rename columns in Camel Case to Snake Case\n",
    "        data.columns = [camel_to_snake(column) for column in data.columns]\n",
    "        \n",
    "        # Add three assertion to check the transformations      \n",
    "        assert all(is_snake_case(column) for column in data.columns), \"Not all column names are in snake case.\"\n",
    "        assert (data['passenger_count'] > 0).all(), \"'passenger_count' contains non-positive values.\"\n",
    "        assert (data['trip_distance'] > 0).all(), \"'trip_distance' contains non-positive values.\"\n",
    "        \n",
    "        return data \n",
    "    \n",
    "    \n",
    "    @test\n",
    "    def test_output(output, *args) -> None:\n",
    "        \"\"\"\n",
    "        Template code for testing the output of the block.\n",
    "        \"\"\"\n",
    "        assert output is not None, 'The output is undefined'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Data Exporter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `Data Exporter` block is responsible for writing the data to a destination after the process of transformation. For this block, we will write the data to a Postgres database that is running in another docker container. \n",
    "\n",
    "<center>\n",
    "<img src=\"figures/data-export.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The result of this data exporter block would be the following:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/data-exporter-block.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "This should take the data from the `Data Transform` block, open a connection to Postgres container and export the data to our schema.  The  full code inside this block is:\n",
    "\n",
    "```python\n",
    "    from mage_ai.settings.repo import get_repo_path\n",
    "    from mage_ai.io.config import ConfigFileLoader\n",
    "    from mage_ai.io.postgres import Postgres\n",
    "    from pandas import DataFrame\n",
    "    from os import path\n",
    "\n",
    "    if 'data_exporter' not in globals():\n",
    "        from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "\n",
    "    @data_exporter\n",
    "    def export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Template for exporting data to a PostgreSQL database.\n",
    "        Specify your configuration settings in 'io_config.yaml'.\n",
    "\n",
    "        Docs: https://docs.mage.ai/design/data-loading#postgresql\n",
    "        \"\"\"\n",
    "        schema_name = 'mage'  # Specify the name of the schema to export data to\n",
    "        table_name = 'green_taxi'  # Specify the name of the table to export data to\n",
    "        config_path = path.join(get_repo_path(), 'io_config.yaml')\n",
    "        config_profile = 'dev'\n",
    "\n",
    "        with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n",
    "            loader.export(\n",
    "                df,\n",
    "                schema_name,\n",
    "                table_name,\n",
    "                index=False,  # Specifies whether to include index in exported table\n",
    "                if_exists='replace',  # Specify resolution policy if table name already exists\n",
    "            )\n",
    "```\n",
    "\n",
    "We can test the dataset exported to the postgres database by creating another data loader block for SQL, selecting the dev profile and typing the following query:\n",
    "\n",
    "```sql\n",
    "    SELECT * FROM mage.green_taxi LIMIT 1;\n",
    "```\n",
    "\n",
    "The result should be the following:\n",
    "<center>\n",
    "<img src=\"figures/load-postgres-data.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "At the end, we would see the following pipeline graph showing each block and the connections between them:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/graph-blocks.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "This is a DAG (Directed Acyclic Graph), which is a tree describing our pipeline, its blocks and dependencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. ETL Pipeline with Mage and Google Cloud**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did previously in [here](https://github.com/marcosbenicio/DE-zoomcamp/blob/main/01docker-postgres-terraform/notes-and-homework.ipynb), we can create a service account and generate a key to authenticate with Google Cloud. We then need to create a bucket to store data and access via Mage. \n",
    "\n",
    "**Mage Configuration File**\n",
    "\n",
    "With the service account created, download the key and save it in the Mage project directory. The `.json` will be automatically mounted into the container and we can use it to authenticate with Google Cloud. In the Mage GUI, go to files, open the `io_config.yaml` file and in the `default` profile search for the Google block:\n",
    "\n",
    "```yaml\n",
    "  # Google\n",
    "  GOOGLE_SERVICE_ACC_KEY:\n",
    "    type: service_account\n",
    "    project_id: project-id\n",
    "    private_key_id: key-id\n",
    "    private_key: \"-----BEGIN PRIVATE KEY-----\\nyour_private_key\\n-----END_PRIVATE_KEY\"\n",
    "    client_email: your_service_account_email\n",
    "    auth_uri: \"https://accounts.google.com/o/oauth2/auth\"\n",
    "    token_uri: \"https://accounts.google.com/o/oauth2/token\"\n",
    "    auth_provider_x509_cert_url: \"https://www.googleapis.com/oauth2/v1/certs\"\n",
    "    client_x509_cert_url: \"https://www.googleapis.com/robot/v1/metadata/x509/your_service_account_email\"\n",
    "  GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"/path/to/your/service/account/key.json\"\n",
    "  GOOGLE_LOCATION: US # Optional\n",
    "```\n",
    "We can add the variables from `GOOGLE_SERVICE_ACC_KEY` or just specify the path to the `.json` file inside the container. Let's choose the second option and add the following configuration to the file and exclude the `GOOGLE_SERVICE_ACC_KEY`:\n",
    "\n",
    "```yaml\n",
    "  # Google\n",
    "  GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"/home/src/mage-413610-2ec7db0afcf7.json\"\n",
    "  GOOGLE_LOCATION: US # Optional\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Export data to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "We can now use the created pipeline `green_taxi_etl` to add a data exporter block named `ingest_green_taxi_gcs` to export the data to Google Cloud Storage. In the green_taxi_etl pipeline we select `Python > Google Cloud Storage` when creating the data exporter. The code for the block is:\n",
    "\n",
    "```python\n",
    "  from mage_ai.settings.repo import get_repo_path\n",
    "  from mage_ai.io.config import ConfigFileLoader\n",
    "  from mage_ai.io.google_cloud_storage import GoogleCloudStorage\n",
    "  from pandas import DataFrame\n",
    "  from os import path\n",
    "  import pyarrow as pa\n",
    "  import pyarrow.parquet as pq\n",
    "  import os\n",
    "\n",
    "  if 'data_exporter' not in globals():\n",
    "      from mage_ai.data_preparation.decorators import data_exporter\n",
    "  # Set the environment variable for Google Cloud authentication\n",
    "  os.evitoment['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/mage-413610-2ec7db0afcf7.json'\n",
    "\n",
    "  # Define the bucket name, project ID, and table name for Google Cloud Storage\n",
    "  bucket_name = 'mage-ingest-413610'\n",
    "  project_id = '413610'\n",
    "  table_name = 'nyc_green_taxi'\n",
    "\n",
    "  # Construct the root path for the data to be stored in GCS\n",
    "  # GCS follow file system structure gs://bucket_name/root_path\n",
    "  root_path = f'{bucket_name}/{table_name}\n",
    "\n",
    "  @data_exporter\n",
    "  def export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n",
    "      \"\"\"\n",
    "      Template for exporting data to a Google Cloud Storage bucket.\n",
    "      Specify your configuration settings in 'io_config.yaml'.\n",
    "\n",
    "      Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n",
    "      \"\"\"\n",
    "      # Convert the DataFrame to a PyArrow Table for efficient storage\n",
    "      table = pa.Table.from_pandas(data)\n",
    "      # Initialize Google Cloud Storage file syste\n",
    "      gcs = pa.fs.GcsFileSystem()\n",
    "\n",
    "      # Write the Table as a Parquet dataset to the specified path in GCS, partitioned by 'lpep_pickup_datetime'\n",
    "      pq.write_to_dataset(\n",
    "          table,\n",
    "          root_path = root_path,\n",
    "          partition_cols =  ['lpep_pickup_date'],\n",
    "          filesystem = gcs\n",
    "      )\n",
    "```\n",
    "We should have a tree for this pipeline as follows:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/ingest-data-gcs-tree.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The data in this pipeline is being loaded from an API, transformed to handle some inconsistencies in the dataset and then exported to Google Cloud Storage and Postgres database. We then load the data from Postgres to check if the data was correctly ingested in the block `load_green_taxi_postgres`. To check the google cloud storage we should see the ingested data partitioned by date in the `google cloud storage > bucket` as follows:\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/gcs-ingested-data.png\" alt=\"drawing\"/>\n",
    "</center>\n",
    "\n",
    "The data has a total of 96 parquet files, one for each date. This is a good practice to optimize the query performance when we need to access the data and ingest a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Homework**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question 1. Data Loading\n",
    "\n",
    "Once the dataset is loaded, what's the shape of the data?\n",
    "\n",
    "* **266,855 rows x 20 columns**\n",
    "* 544,898 rows x 18 columns\n",
    "* 544,898 rows x 20 columns\n",
    "* 133,744 rows x 20 columns\n",
    "\n",
    " Question 2. Data Transformation\n",
    "\n",
    "Upon filtering the dataset where the passenger count is greater than 0 _and_ the trip distance is greater than zero, how many rows are left?\n",
    "\n",
    "* 544,897 rows\n",
    "* 266,855 rows\n",
    "* **139,370 rows**\n",
    "* 266,856 rows\n",
    "\n",
    "- Question 3. Data Transformation\n",
    "\n",
    "Which of the following creates a new column `lpep_pickup_date` by converting `lpep_pickup_datetime` to a date?\n",
    "\n",
    "* `data = data['lpep_pickup_datetime'].date`\n",
    "* `data('lpep_pickup_date') = data['lpep_pickup_datetime'].date`\n",
    "* **`data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt.date`**\n",
    "* `data['lpep_pickup_date'] = data['lpep_pickup_datetime'].dt().date()`\n",
    "\n",
    "- Question 4. Data Transformation\n",
    "\n",
    "What are the existing values of `VendorID` in the dataset?\n",
    "\n",
    "* 1, 2, or 3\n",
    "* **1 or 2**\n",
    "* 1, 2, 3, 4\n",
    "* 1\n",
    "\n",
    "- Question 5. Data Transformation\n",
    "\n",
    "How many columns need to be renamed to snake case?\n",
    "\n",
    "* 3\n",
    "* 6\n",
    "* 2\n",
    "* **4**\n",
    "\n",
    "- Question 6. Data Exporting\n",
    "\n",
    "Once exported, how many partitions (folders) are present in Google Cloud?\n",
    "\n",
    "* **96**\n",
    "* 56\n",
    "* 67\n",
    "* 108"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-workflow-orchestration-0nfOykv9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
